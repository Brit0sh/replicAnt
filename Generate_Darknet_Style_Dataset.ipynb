{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pathlib\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55 files...\n",
      "reading ColonieInfo.csv\n",
      "Loaded colony file with seed  14256 and 100 individuals.\n",
      "\n",
      "A total of 4 unique classes have been found.\n",
      "The classes and respective class IDs are:\n",
      " {'00011': 0, '00262': 1, '00501': 2, 'Sungaya': 3}\n"
     ]
    }
   ],
   "source": [
    "# define location of dataset and return all files\n",
    "dataset_location = \"C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output\"\n",
    "target_dir = \"C:/Users/Legos/Documents/PhD/FARTS/generated_data/YOLO_latest_3D_synth_test\"\n",
    "all_files = [f for f in listdir(dataset_location) if isfile(join(dataset_location, f))]\n",
    "\n",
    "# next, sort files into images, depth maps, segmentation maps, data, and colony info\n",
    "dataset_img = []\n",
    "dataset_depth = []\n",
    "dataset_seg = []\n",
    "dataset_data = []\n",
    "dataset_colony = dataset_location + \"/ColonieInfo.csv\"\n",
    "\n",
    "for file in all_files:\n",
    "    loc = dataset_location + \"/\" + file\n",
    "    if file[-7:-4] == \"Img\":\n",
    "        dataset_img.append(loc)\n",
    "    elif file[-7:-4] == \"Seg\":\n",
    "        dataset_seg.append(loc)\n",
    "    elif file[-8:-4] == \"Depth\":\n",
    "        dataset_depth.append(loc)\n",
    "    elif file[-8:-4] == \"Data\":\n",
    "        dataset_data.append(loc)\n",
    "        \n",
    "print(\"Found\",len(all_files),\"files...\")\n",
    "\n",
    "# next sort the colony info into its IDs to determine the colony size and individual scales\n",
    "# one entry for each successive ID is read\n",
    "from csv import reader\n",
    "\n",
    "colony = {'seed': 0,\n",
    "            'ID': [],\n",
    "         'scale': [],\n",
    "        'weight': []}\n",
    "\n",
    "with open(dataset_colony, 'r') as colony_file:\n",
    "        print(\"reading\", file)\n",
    "        # pass the file object to reader() to get the reader object\n",
    "        csv_reader = reader(colony_file)\n",
    "        # iterate over each row in the csv using reader object\n",
    "        for r, row in enumerate(csv_reader):\n",
    "            if r == 0:\n",
    "                colony['seed'] = row[0].split(\"=\")[-1]\n",
    "            else:\n",
    "                colony['ID'].append(row[0].split(\"=\")[-1])\n",
    "                colony['weight'].append(row[1].split(\"_\")[1])\n",
    "                colony['scale'].append(float(row[2].split(\"=\")[-1]))\n",
    "\n",
    "print(\"Loaded colony file with seed\",colony['seed'],\"and\",len(colony['ID']),\"individuals.\")\n",
    "\n",
    "# get provided classes to create a dictionary of class IDs and class names\n",
    "subject_class_names = np.unique(np.array(colony[\"weight\"]))\n",
    "subject_classes = {}\n",
    "for id,sbj in enumerate(subject_class_names):\n",
    "    subject_classes[str(sbj)] = id\n",
    "\n",
    "print(\"\\nA total of\",len(subject_class_names),\"unique classes have been found.\")\n",
    "print(\"The classes and respective class IDs are:\\n\",subject_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the cleaned colony info, we can start loading the data associated with each frame.\n",
    "For simplicity we will simply this a list of list as the number of individuals.\n",
    "\n",
    "We will therefore access \"data\" as [frame] [individual] [attribute], where attributes will include [ID,bbox_x_0,bbox_y_0,...]\n",
    "\n",
    "for now training and evaluating detectors, only the bounding box (and ID) info will be relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/10_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/11_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/12_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/13_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/14_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/15_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/16_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/17_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/18_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/1_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/2_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/3_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/4_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/5_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/6_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/7_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/8_Data.csv\n",
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/9_Data.csv\n",
      "\n",
      "The dataset has a total of 18 generated frames.\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for file in dataset_data:\n",
    "    # store all returned coordinates for each individual\n",
    "    coords = []\n",
    "        \n",
    "    # open file in read mode\n",
    "    with open(file, 'r') as read_obj:\n",
    "        print(\"reading\", file)\n",
    "        # pass the file object to reader() to get the reader object\n",
    "        csv_reader = reader(read_obj)\n",
    "        # iterate over each row in the csv using reader object\n",
    "        for row in csv_reader:\n",
    "            # exclude camera projection row\n",
    "            if not row[0].split(\".\")[0] == \"camera_projection:\":\n",
    "                individual = [float(row[0].split(\".\")[0])]\n",
    "                # row variable is a list that represents a row in csv\n",
    "                for elem in row:\n",
    "                    try:\n",
    "                        individual.append(float(elem.split(\"=\")[-1]))\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                coords.append(individual)\n",
    "        \n",
    "    data.append(coords)\n",
    "    \n",
    "print(\"\\nThe dataset has a total of\", len(data),\"generated frames.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there may be animals for which we don't use all bones we can return a list of all labels and exclude the respective locations from the pose data. As all animals use the same convention, we can simply read in one example and remove the corresponding indices from all animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading C:/Users/Legos/Documents/PhD/FARTS/UNREAL/FARTS_STICKS/Output/9_Data.csv\n",
      "\n",
      "Corresponding to the following indices: [313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352]\n",
      "\n",
      "All labels:\n",
      "['5.BoundingBox.BoundMin.X', 'BoundingBox.BoundMin.Y', 'BoundingBox.BoundMax.X', 'BoundingBox.BoundMax.Y', 'b_t.X', 'b_t.Y', 'b_t.X_world', 'b_t.Y_world', 'b_t.Z_world', 'b_a_1.X', 'b_a_1.Y', 'b_a_1.X_world', 'b_a_1.Y_world', 'b_a_1.Z_world', 'b_a_2.X', 'b_a_2.Y', 'b_a_2.X_world', 'b_a_2.Y_world', 'b_a_2.Z_world', 'b_a_3.X', 'b_a_3.Y', 'b_a_3.X_world', 'b_a_3.Y_world', 'b_a_3.Z_world', 'b_a_4.X', 'b_a_4.Y', 'b_a_4.X_world', 'b_a_4.Y_world', 'b_a_4.Z_world', 'b_a_5.X', 'b_a_5.Y', 'b_a_5.X_world', 'b_a_5.Y_world', 'b_a_5.Z_world', 'b_a_5_end.X', 'b_a_5_end.Y', 'b_a_5_end.X_world', 'b_a_5_end.Y_world', 'b_a_5_end.Z_world', 'l_1_co_r.X', 'l_1_co_r.Y', 'l_1_co_r.X_world', 'l_1_co_r.Y_world', 'l_1_co_r.Z_world', 'l_1_tr_r.X', 'l_1_tr_r.Y', 'l_1_tr_r.X_world', 'l_1_tr_r.Y_world', 'l_1_tr_r.Z_world', 'l_1_fe_r.X', 'l_1_fe_r.Y', 'l_1_fe_r.X_world', 'l_1_fe_r.Y_world', 'l_1_fe_r.Z_world', 'l_1_ti_r.X', 'l_1_ti_r.Y', 'l_1_ti_r.X_world', 'l_1_ti_r.Y_world', 'l_1_ti_r.Z_world', 'l_1_ta_r.X', 'l_1_ta_r.Y', 'l_1_ta_r.X_world', 'l_1_ta_r.Y_world', 'l_1_ta_r.Z_world', 'l_1_pt_r.X', 'l_1_pt_r.Y', 'l_1_pt_r.X_world', 'l_1_pt_r.Y_world', 'l_1_pt_r.Z_world', 'l_1_pt_r_end.X', 'l_1_pt_r_end.Y', 'l_1_pt_r_end.X_world', 'l_1_pt_r_end.Y_world', 'l_1_pt_r_end.Z_world', 'l_2_co_r.X', 'l_2_co_r.Y', 'l_2_co_r.X_world', 'l_2_co_r.Y_world', 'l_2_co_r.Z_world', 'l_2_tr_r.X', 'l_2_tr_r.Y', 'l_2_tr_r.X_world', 'l_2_tr_r.Y_world', 'l_2_tr_r.Z_world', 'l_2_fe_r.X', 'l_2_fe_r.Y', 'l_2_fe_r.X_world', 'l_2_fe_r.Y_world', 'l_2_fe_r.Z_world', 'l_2_ti_r.X', 'l_2_ti_r.Y', 'l_2_ti_r.X_world', 'l_2_ti_r.Y_world', 'l_2_ti_r.Z_world', 'l_2_ta_r.X', 'l_2_ta_r.Y', 'l_2_ta_r.X_world', 'l_2_ta_r.Y_world', 'l_2_ta_r.Z_world', 'l_2_pt_r.X', 'l_2_pt_r.Y', 'l_2_pt_r.X_world', 'l_2_pt_r.Y_world', 'l_2_pt_r.Z_world', 'l_2_pt_r_end.X', 'l_2_pt_r_end.Y', 'l_2_pt_r_end.X_world', 'l_2_pt_r_end.Y_world', 'l_2_pt_r_end.Z_world', 'l_3_co_r.X', 'l_3_co_r.Y', 'l_3_co_r.X_world', 'l_3_co_r.Y_world', 'l_3_co_r.Z_world', 'l_3_tr_r.X', 'l_3_tr_r.Y', 'l_3_tr_r.X_world', 'l_3_tr_r.Y_world', 'l_3_tr_r.Z_world', 'l_3_fe_r.X', 'l_3_fe_r.Y', 'l_3_fe_r.X_world', 'l_3_fe_r.Y_world', 'l_3_fe_r.Z_world', 'l_3_ti_r.X', 'l_3_ti_r.Y', 'l_3_ti_r.X_world', 'l_3_ti_r.Y_world', 'l_3_ti_r.Z_world', 'l_3_ta_r.X', 'l_3_ta_r.Y', 'l_3_ta_r.X_world', 'l_3_ta_r.Y_world', 'l_3_ta_r.Z_world', 'l_3_pt_r.X', 'l_3_pt_r.Y', 'l_3_pt_r.X_world', 'l_3_pt_r.Y_world', 'l_3_pt_r.Z_world', 'l_3_pt_r_end.X', 'l_3_pt_r_end.Y', 'l_3_pt_r_end.X_world', 'l_3_pt_r_end.Y_world', 'l_3_pt_r_end.Z_world', 'l_1_co_l.X', 'l_1_co_l.Y', 'l_1_co_l.X_world', 'l_1_co_l.Y_world', 'l_1_co_l.Z_world', 'l_1_tr_l.X', 'l_1_tr_l.Y', 'l_1_tr_l.X_world', 'l_1_tr_l.Y_world', 'l_1_tr_l.Z_world', 'l_1_fe_l.X', 'l_1_fe_l.Y', 'l_1_fe_l.X_world', 'l_1_fe_l.Y_world', 'l_1_fe_l.Z_world', 'l_1_ti_l.X', 'l_1_ti_l.Y', 'l_1_ti_l.X_world', 'l_1_ti_l.Y_world', 'l_1_ti_l.Z_world', 'l_1_ta_l.X', 'l_1_ta_l.Y', 'l_1_ta_l.X_world', 'l_1_ta_l.Y_world', 'l_1_ta_l.Z_world', 'l_1_pt_l.X', 'l_1_pt_l.Y', 'l_1_pt_l.X_world', 'l_1_pt_l.Y_world', 'l_1_pt_l.Z_world', 'l_1_pt_l_end.X', 'l_1_pt_l_end.Y', 'l_1_pt_l_end.X_world', 'l_1_pt_l_end.Y_world', 'l_1_pt_l_end.Z_world', 'l_2_co_l.X', 'l_2_co_l.Y', 'l_2_co_l.X_world', 'l_2_co_l.Y_world', 'l_2_co_l.Z_world', 'l_2_tr_l.X', 'l_2_tr_l.Y', 'l_2_tr_l.X_world', 'l_2_tr_l.Y_world', 'l_2_tr_l.Z_world', 'l_2_fe_l.X', 'l_2_fe_l.Y', 'l_2_fe_l.X_world', 'l_2_fe_l.Y_world', 'l_2_fe_l.Z_world', 'l_2_ti_l.X', 'l_2_ti_l.Y', 'l_2_ti_l.X_world', 'l_2_ti_l.Y_world', 'l_2_ti_l.Z_world', 'l_2_ta_l.X', 'l_2_ta_l.Y', 'l_2_ta_l.X_world', 'l_2_ta_l.Y_world', 'l_2_ta_l.Z_world', 'l_2_pt_l.X', 'l_2_pt_l.Y', 'l_2_pt_l.X_world', 'l_2_pt_l.Y_world', 'l_2_pt_l.Z_world', 'l_2_pt_l_end.X', 'l_2_pt_l_end.Y', 'l_2_pt_l_end.X_world', 'l_2_pt_l_end.Y_world', 'l_2_pt_l_end.Z_world', 'l_3_co_l.X', 'l_3_co_l.Y', 'l_3_co_l.X_world', 'l_3_co_l.Y_world', 'l_3_co_l.Z_world', 'l_3_tr_l.X', 'l_3_tr_l.Y', 'l_3_tr_l.X_world', 'l_3_tr_l.Y_world', 'l_3_tr_l.Z_world', 'l_3_fe_l.X', 'l_3_fe_l.Y', 'l_3_fe_l.X_world', 'l_3_fe_l.Y_world', 'l_3_fe_l.Z_world', 'l_3_ti_l.X', 'l_3_ti_l.Y', 'l_3_ti_l.X_world', 'l_3_ti_l.Y_world', 'l_3_ti_l.Z_world', 'l_3_ta_l.X', 'l_3_ta_l.Y', 'l_3_ta_l.X_world', 'l_3_ta_l.Y_world', 'l_3_ta_l.Z_world', 'l_3_pt_l.X', 'l_3_pt_l.Y', 'l_3_pt_l.X_world', 'l_3_pt_l.Y_world', 'l_3_pt_l.Z_world', 'l_3_pt_l_end.X', 'l_3_pt_l_end.Y', 'l_3_pt_l_end.X_world', 'l_3_pt_l_end.Y_world', 'l_3_pt_l_end.Z_world', 'b_h.X', 'b_h.Y', 'b_h.X_world', 'b_h.Y_world', 'b_h.Z_world', 'ma_r.X', 'ma_r.Y', 'ma_r.X_world', 'ma_r.Y_world', 'ma_r.Z_world', 'ma_r_end.X', 'ma_r_end.Y', 'ma_r_end.X_world', 'ma_r_end.Y_world', 'ma_r_end.Z_world', 'an_1_r.X', 'an_1_r.Y', 'an_1_r.X_world', 'an_1_r.Y_world', 'an_1_r.Z_world', 'an_2_r.X', 'an_2_r.Y', 'an_2_r.X_world', 'an_2_r.Y_world', 'an_2_r.Z_world', 'an_3_r.X', 'an_3_r.Y', 'an_3_r.X_world', 'an_3_r.Y_world', 'an_3_r.Z_world', 'an_3_r_end.X', 'an_3_r_end.Y', 'an_3_r_end.X_world', 'an_3_r_end.Y_world', 'an_3_r_end.Z_world', 'ma_l.X', 'ma_l.Y', 'ma_l.X_world', 'ma_l.Y_world', 'ma_l.Z_world', 'ma_l_end.X', 'ma_l_end.Y', 'ma_l_end.X_world', 'ma_l_end.Y_world', 'ma_l_end.Z_world', 'an_1_l.X', 'an_1_l.Y', 'an_1_l.X_world', 'an_1_l.Y_world', 'an_1_l.Z_world', 'an_2_l.X', 'an_2_l.Y', 'an_2_l.X_world', 'an_2_l.Y_world', 'an_2_l.Z_world', 'an_3_l.X', 'an_3_l.Y', 'an_3_l.X_world', 'an_3_l.Y_world', 'an_3_l.Z_world', 'an_3_l_end.X', 'an_3_l_end.Y', 'an_3_l_end.X_world', 'an_3_l_end.Y_world', 'an_3_l_end.Z_world', 'w_1_r.X', 'w_1_r.Y', 'w_1_r.X_world', 'w_1_r.Y_world', 'w_1_r.Z_world', 'w_1_r_end.X', 'w_1_r_end.Y', 'w_1_r_end.X_world', 'w_1_r_end.Y_world', 'w_1_r_end.Z_world', 'w_2_r.X', 'w_2_r.Y', 'w_2_r.X_world', 'w_2_r.Y_world', 'w_2_r.Z_world', 'w_2_r_end.X', 'w_2_r_end.Y', 'w_2_r_end.X_world', 'w_2_r_end.Y_world', 'w_2_r_end.Z_world', 'w_1_l.X', 'w_1_l.Y', 'w_1_l.X_world', 'w_1_l.Y_world', 'w_1_l.Z_world', 'w_1_l_end.X', 'w_1_l_end.Y', 'w_1_l_end.X_world', 'w_1_l_end.Y_world', 'w_1_l_end.Z_world', 'w_2_l.X', 'w_2_l.Y', 'w_2_l.X_world', 'w_2_l.Y_world', 'w_2_l.Z_world', 'w_2_l_end.X', 'w_2_l_end.Y', 'w_2_l_end.X_world', 'w_2_l_end.Y_world', 'w_2_l_end.Z_world']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# first open and read the first line from the first imported data file\n",
    "labels = []\n",
    "entries_found = False\n",
    "entry = 0\n",
    "\n",
    "while not entries_found:\n",
    "    with open(dataset_data[entry], 'r') as read_obj:\n",
    "        print(\"reading\", read_obj.name)\n",
    "        # pass the file object to reader() to get the reader object\n",
    "        csv_reader = reader(read_obj)\n",
    "        row_0 = next(csv_reader)  # gets the first line\n",
    "        # iterate over each row in the csv using reader object\n",
    "        if row_0[0][:3] != \"cam\":\n",
    "            entries_found = True\n",
    "            for elem in row_0:\n",
    "                try:\n",
    "                    labels.append((elem.split(\"=\")[0].split(\"Bone.\")[-1]))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        else:\n",
    "            print(\"No entries found! Reading next file... \\n\")\n",
    "            entry += 1\n",
    "\n",
    "# show all used labels:\n",
    "print(\"\\nAll labels:\")\n",
    "print(labels)\n",
    "\"\"\"\n",
    "\n",
    "# first open and read the first line from the first imported data file\n",
    "labels = []\n",
    "with open(dataset_data[0], 'r') as read_obj:\n",
    "    print(\"reading\", file)\n",
    "    # pass the file object to reader() to get the reader object\n",
    "    csv_reader = reader(read_obj)\n",
    "    row_0 = next(csv_reader)  # gets the first line\n",
    "    # iterate over each row in the csv using reader object\n",
    "    for elem in row_0:\n",
    "        try:\n",
    "            labels.append((elem.split(\"=\")[0].split(\"Bone.\")[-1]))\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "# now let's define which labels NOT to use (in our case, all labels relating to wings)\n",
    "# ... so that just means \"omit all lables that start with 'w'\"\n",
    "\n",
    "matched_labels = []# [i for i, item in enumerate(labels) if item in omit_labels]\n",
    "for l, label in enumerate(labels):\n",
    "    if label[0] == \"w\":\n",
    "        matched_labels.append(l-1)\n",
    "        \n",
    "print(\"\\nCorresponding to the following indices:\",matched_labels)\n",
    "\n",
    "# time to remove them from all individuals in \"data\"\n",
    "for f, frame in enumerate(data):\n",
    "    for i, individual in enumerate(frame):\n",
    "        ind_temp = np.array(data[f][i])\n",
    "        data[f][i] = np.delete(ind_temp, matched_labels)\n",
    "\n",
    "\n",
    "# show all used labels:\n",
    "print(\"\\nAll labels:\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded data and colony info we can start plotting bounding boxes on top of their respective images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform between sRGB and linear colour space (optional)\n",
    "\n",
    "def to_linear(srgb):\n",
    "    linear = np.float32(srgb) / 255.0\n",
    "    less = linear <= 0.04045\n",
    "    linear[less] = linear[less] / 12.92\n",
    "    linear[~less] = np.power((linear[~less] + 0.055) / 1.055, 2.4)\n",
    "    return linear * 255.0\n",
    "\n",
    "    \n",
    "def from_linear(linear):\n",
    "    srgb = linear.copy()\n",
    "    less = linear <= 0.0031308\n",
    "    srgb[less] = linear[less] * 12.92\n",
    "    srgb[~less] = 1.055 * np.power(linear[~less], 1.0 / 2.4) - 0.055\n",
    "    return srgb * 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 12 threads to parse data...\n",
      "Using custom labels: ['00011' '00262' '00501' 'Sungaya']\n",
      "Using 16 training images and 1 test images. (10.0 %)\n",
      "Successfully created all required files!\n",
      "Starting Thread_0\n",
      "Starting Thread_1\n",
      "Starting Thread_2\n",
      "Starting Thread_3\n",
      "Starting Thread_4\n",
      "Starting Thread_5\n",
      "Starting Thread_6\n",
      "Starting Thread_7\n",
      "Starting Thread_8\n",
      "Starting Thread_9\n",
      "Starting Thread_10\n",
      "Starting Thread_11\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "Exiting Thread_11\n",
      "Exiting Thread_8\n",
      "Exiting Thread_7\n",
      "Exiting Thread_6\n",
      "Exiting Thread_3\n",
      "Exiting Thread_2\n",
      "Exiting Thread_5\n",
      "Exiting Thread_4\n",
      "Exiting Thread_9\n",
      "Exiting Thread_1\n",
      "Exiting Thread_10\n",
      "Exiting Thread_0\n",
      "Exiting Main Stacking Thread\n",
      "Using 16 training images and 1 test images. (10.0 %)\n",
      "Successfully created all required files!\n",
      "Total time elapsed: 3.6305675506591797 seconds\n"
     ]
    }
   ],
   "source": [
    "# create unique colours for each ID\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# alright. Let's take it from the top and fucking multi-thread this.\n",
    "import threading\n",
    "import queue\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def getThreads():\n",
    "    \"\"\" Returns the number of available threads on a posix/win based system \"\"\"\n",
    "    if sys.platform == 'win32':\n",
    "        return int(os.environ['NUMBER_OF_PROCESSORS'])\n",
    "    else:\n",
    "        return int(os.popen('grep -c cores /proc/cpuinfo').read())\n",
    "\n",
    "class myThread(threading.Thread):\n",
    "    def __init__(self, threadID, name, q):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.threadID = threadID\n",
    "        self.name = name\n",
    "        self.q = q\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Starting \" + self.name)\n",
    "        process_detections(self.name, self.q)\n",
    "        print(\"Exiting \" + self.name)\n",
    "        \n",
    "def createThreadList(num_threads):\n",
    "    threadNames = []\n",
    "    for t in range(num_threads):\n",
    "        threadNames.append(\"Thread_\" + str(t))\n",
    "\n",
    "    return threadNames\n",
    "\n",
    "def process_detections(threadName, q):\n",
    "    while not exitFlag_stacking:\n",
    "        queueLock.acquire()\n",
    "        if not workQueue_stacking.empty():\n",
    "            \n",
    "            data_input = q.get()\n",
    "            i = data_input[0]\n",
    "            print(i)\n",
    "            img = data_input[1]\n",
    "            queueLock.release()\n",
    "            \n",
    "            display_img = cv2.imread(img)\n",
    "            display_img_out = display_img.copy()\n",
    "\n",
    "            if generate_dataset:\n",
    "                img_info = []\n",
    "                \n",
    "            # compute visibility for each individual\n",
    "            seg_img = cv2.imread(dataset_seg[i])\n",
    "            \n",
    "            individual_visible = False\n",
    "\n",
    "            for im, individual in enumerate(data[i]):\n",
    "\n",
    "                fontColor = (int(ID_colours[int(individual[0]),0]),\n",
    "                             int(ID_colours[int(individual[0]),1]),\n",
    "                             int(ID_colours[int(individual[0]),2]))\n",
    "                if enforce_tight_bboxes:\n",
    "                    bbox = fix_bounding_boxes(individual[1:],max_val=display_img.shape)\n",
    "                else:\n",
    "                    bbox = fix_bounding_boxes(individual[1:5],max_val=display_img.shape)\n",
    "\n",
    "                # FOR SOME REASON, OCCASIONALLY, THE ID OF THE SEG FILE IS LOWER THAN THE DATA FILE\n",
    "                # with: ID = red_channel/255 * 50\n",
    "                # red_channel = (ID/50) * 255\n",
    "                ID_red_val = int((individual[0]/len(colony['ID']))*255)\n",
    "                try:\n",
    "                    ID_mask = cv2.inRange(seg_img[bbox[1]:bbox[3],bbox[0]:bbox[2]], np.array([0,0, ID_red_val - 5]), np.array([0,0, ID_red_val + 5]))\n",
    "                    indivual_occupancy = cv2.countNonZero(ID_mask)\n",
    "                except:\n",
    "                    if len(threadList_stacking) == 1: \n",
    "                        print(\"Individual fully occluded:\",im,\"in\",dataset_seg[i])\n",
    "                    indivual_occupancy = 1\n",
    "                \n",
    "                #indivual_occupancy = np.count_nonzero((seg_img == [0, 0, int((individual[0]/len(colony['ID']))*255)]).all(axis = 2)) + np.count_nonzero((seg_img == [0, 0, int((individual[0]/len(colony['ID']))*255 - 1)]).all(axis = 2)) + np.count_nonzero((seg_img == [0, 0, int((individual[0]/len(colony['ID']))*255 + 1)]).all(axis = 2))\n",
    "                bbox_area = abs((bbox[2] - bbox[0]) * (bbox[3] - bbox[1])) + 1\n",
    "                bbox_occupancy = indivual_occupancy / bbox_area\n",
    "                #print(\"Individual\", individual[0], \"with bounding box occupancy \",bbox_occupancy)\n",
    "\n",
    "                cv2.putText(display_img, \"ID: \" + str(int(individual[0])), (bbox[0] + 10,bbox[3] - 10), font, fontScale, fontColor, lineType)\n",
    "                class_ID = subject_classes[colony['weight'][int(individual[0])]] # here we use a single class, otherwise this can be replaced by size / scale values\n",
    "                        \n",
    "                if bbox_occupancy > visibility_threshold:\n",
    "                    \n",
    "                    individual_visible = True\n",
    "                            \n",
    "                    if generate_dataset:\n",
    "                        # now we need to convert the bounding box info into the desired format.\n",
    "                        img_dim = display_img.shape\n",
    "\n",
    "                        # [class_ID, centre_x, centre_y, bounding_box_width, bounding_box_height]\n",
    "                        valid_new_x = False\n",
    "                        valid_new_y = False\n",
    "                        \n",
    "                        if enforce_centred_bboxes:\n",
    "                            # coords of head\n",
    "                            b_t = np.array([individual[5], individual[6]])\n",
    "                            # coords of abdomen\n",
    "                            b_a_1 = np.array([individual[10], individual[11]])\n",
    "                            # compute new centre point\n",
    "                            new_centre_x = (individual[5] + individual[10]) / 2\n",
    "                            new_centre_y = (individual[6] + individual[11]) / 2\n",
    "                            \n",
    "                            if new_centre_x < img_dim[1] and new_centre_x > 0:\n",
    "                                centre_x = new_centre_x / img_dim[1]\n",
    "                                valid_new_x = True\n",
    "                            \n",
    "                            if new_centre_y < img_dim[0] and new_centre_y > 0:\n",
    "                                centre_y = new_centre_y / img_dim[0]\n",
    "                                valid_new_y = True\n",
    "                            \n",
    "                            cv2.circle(display_img, (int(new_centre_x),int(new_centre_y)), \n",
    "                                       radius=3, color=fontColor, thickness=-1)    \n",
    "                        \n",
    "                        for label in range(int((len(individual)-5)/5)):\n",
    "                            cv2.circle(display_img, (int(individual[label*5+5]),\n",
    "                                                     int(individual[label*5+6])), \n",
    "                                       radius=3, color=fontColor, thickness=-1)    \n",
    "    \n",
    "                        \n",
    "                        bounding_box_width = abs(bbox[2] - bbox[0]) / img_dim[1]\n",
    "                        bounding_box_height = abs(bbox[3] - bbox[1]) / img_dim[0]\n",
    "                        \n",
    "                        if not valid_new_x or not valid_new_y:\n",
    "                            centre_x = bbox[0] / img_dim[1] + bounding_box_width / 2\n",
    "                            centre_y = bbox[1] / img_dim[0] + bounding_box_height / 2\n",
    "                            \n",
    "                        img_info.append([class_ID,centre_x,centre_y,bounding_box_width,bounding_box_height])\n",
    "                        \n",
    "                        cv2.rectangle(display_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), fontColor, 2)\n",
    "                        \n",
    "                else:\n",
    "                    pass\n",
    "                    if len(threadList_stacking) == 1: \n",
    "                        print(\"Ah shit, can't see\",int(individual[0]),class_ID)\n",
    "\n",
    "                if generate_dataset and individual_visible:\n",
    "                    \n",
    "                    img_name = target_dir + \"/obj/\" + img.split('/')[-1][:-4] + \"_synth\" + \".JPG\"\n",
    "                    cv2.imwrite(img_name, display_img_out)\n",
    "                    \n",
    "                    with open(target_dir + \"/obj/\" + img.split('/')[-1][:-4] + \"_synth\" + \".txt\", \"w\") as f: \n",
    "                        output_txt = []\n",
    "                        if img_info:\n",
    "                            for line in img_info:\n",
    "                                line_str = ' '.join([str(i) for i in line])\n",
    "                                output_txt.append(line_str+\"\\n\")\n",
    "                            f.writelines(output_txt)\n",
    "                        else:\n",
    "                            f.write(\"\")\n",
    "                            \n",
    "            if len(threadList_stacking) == 1:\n",
    "                cv2.imshow(\"labeled image\", cv2.resize(display_img, (int(display_img.shape[1] / 2), \n",
    "                                                                     int(display_img.shape[0] / 2))))\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "        else:\n",
    "            queueLock.release()\n",
    "            \n",
    "            \n",
    "####################################################################################################\n",
    "            \n",
    "# set True to show processing results for each image (disables parallel processing)\n",
    "DEBUG = False\n",
    "\n",
    "# we can additionally plot the points in the data files to check joint locations\n",
    "# WARNING: At the moment there seems to be an issue with inccorrectly given joint locations\n",
    "plot_joints = False\n",
    "\n",
    "# remember to refine an export folder when saving out your dataset\n",
    "generate_dataset = True\n",
    "\n",
    "# we can enforce the bounding box to centre on the individual instead of being influenced by its orientation\n",
    "# As the groundtruth in real recordings is annotated in the same way this should boost the average accuracy\n",
    "enforce_centred_bboxes = False\n",
    "\n",
    "# alternatively, we can draw tighter bounding boxes without enforced centres, based on 2D keypoints\n",
    "enforce_tight_bboxes = True #one or the other. This option will overwrite \"enforce_centred\" if True\n",
    "\n",
    "####################################################################################################\n",
    "            \n",
    "# setup as many threads as there are (virtual) CPUs\n",
    "exitFlag_stacking = 0\n",
    "# only use a fourth of the number of CPUs for stacking as hugin and enfuse utilise multi core processing in part\n",
    "if DEBUG:\n",
    "    threadList_stacking = createThreadList(1)\n",
    "else:\n",
    "    threadList_stacking = createThreadList(getThreads())\n",
    "print(\"Using\", len(threadList_stacking), \"threads to parse data...\")\n",
    "queueLock = threading.Lock()\n",
    "\n",
    "# define paths to all images and set the maximum number of items in the queue equivalent to the number of images\n",
    "workQueue_stacking = queue.Queue(len(dataset_img))\n",
    "threads = []\n",
    "threadID = 1\n",
    "\n",
    "\n",
    "np.random.seed(seed=1)\n",
    "ID_colours = np.random.randint(255, size=(len(colony['ID']), 3))\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale = 0.5\n",
    "lineType = 2\n",
    "\n",
    "def fix_bounding_boxes(coords,max_val=[1024,1024],exclude_wings=True):\n",
    "    # fix bounding box coordinates so they do not reach beyond the image\n",
    "    # you can either pass only bounding box coordinates or the entire skeleton coordinates\n",
    "    # The latter will recalculate a tighter bounding box, based on all keypoints\n",
    "    # When recalculating the bounding box based on all keypoints, you can chose to ignore wings.\n",
    "    fixed_coords = []\n",
    "    \n",
    "    if len(coords) == 4:\n",
    "        coords_bbox = coords[:4]\n",
    "    \n",
    "    else:\n",
    "        coords_bbox = [0,0,max_val[0],max_val[1]]\n",
    "        # get all X and Y coordinates to find min and max values for the bounding box\n",
    "        key_x = coords[4::5]\n",
    "        key_y = coords[5::5]\n",
    "        \n",
    "        coords_bbox[0] = max([0,min(key_x)])\n",
    "        coords_bbox[1] = max([0,min(key_y)])\n",
    "        coords_bbox[2] = min([max_val[0],max(key_x)])\n",
    "        coords_bbox[3] = min([max_val[1],max(key_y)])\n",
    "    \n",
    "    for c, coord in enumerate(coords_bbox):\n",
    "        if c == 0 or c == 2:\n",
    "            max_val_temp = max_val[1]\n",
    "        else:\n",
    "            max_val_temp = max_val[0]\n",
    "            \n",
    "        if coord >= max_val_temp:\n",
    "            coord = max_val_temp\n",
    "        elif coord <= 0:\n",
    "            coord = 0\n",
    "        \n",
    "        fixed_coords.append(int(coord))\n",
    "        \n",
    "    return fixed_coords\n",
    "\n",
    "if generate_dataset:\n",
    "    from helper.Generate_YOLO_training import createCustomFiles\n",
    "    createCustomFiles(output_folder=target_dir+\"/\",obIDs=subject_class_names)\n",
    "\n",
    "# determine the proportion of a bounding box that needs to be filled before considering the visibility as too low\n",
    "# WARNING: At the moment the ID shown in segmentation maps does not always correspond to the ID in the data file (off by 1)\n",
    "visibility_threshold = 0.015\n",
    "\n",
    "timer = time.time()\n",
    "\n",
    "# Create new threads\n",
    "for tName in threadList_stacking:\n",
    "    thread = myThread(threadID, tName, workQueue_stacking)\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    threadID += 1\n",
    "\n",
    "# Fill the queue with stacks\n",
    "queueLock.acquire()\n",
    "for i,img in enumerate(dataset_img):\n",
    "    workQueue_stacking.put([i, img])\n",
    "queueLock.release()\n",
    "\n",
    "# Wait for queue to empty\n",
    "while not workQueue_stacking.empty():\n",
    "    pass\n",
    "\n",
    "# Notify threads it's time to exit\n",
    "exitFlag_stacking = 1\n",
    "\n",
    "# Wait for all threads to complete\n",
    "for t in threads:\n",
    "    t.join()\n",
    "print(\"Exiting Main Stacking Thread\")\n",
    "\n",
    "# now run createCustomFiles again, to update the train.txt and test.txt files to include the paths to the respective images\n",
    "if generate_dataset:\n",
    "    createCustomFiles(output_folder=target_dir+\"/\")\n",
    "\n",
    "# close all windows if they were opened\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Total time elapsed:\",time.time()-timer,\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, re-run **Generate_YOLO_training()** to combine all files into their final test/train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom labels: ['00011' '00262' '00501' 'Sungaya']\n",
      "Using 16 training images and 1 test images. (10.0 %)\n",
      "Successfully created all required files!\n"
     ]
    }
   ],
   "source": [
    "from helper.Generate_YOLO_training import createCustomFiles\n",
    "createCustomFiles(output_folder=target_dir+\"/\",obIDs=subject_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command to train yolo based on generated data:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
