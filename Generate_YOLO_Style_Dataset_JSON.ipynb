{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pathlib\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "import sys\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 samples...\n",
      "\n",
      "A total of 1 unique classes have been found.\n",
      "The classes and respective class IDs are:\n",
      " {'insect': 0}\n",
      "Loaded colony file with seed 895707840\n"
     ]
    }
   ],
   "source": [
    "# define location of dataset and return all files\n",
    "dataset_location = \"C:/Users/Legos/Documents/PhD/FARTS/FARTS_DATA/AntTest02_RAW\"\n",
    "target_dir = \"C:/Users/Legos/Documents/PhD/FARTS/FARTS_DATA/AntTest02_YOLO\"\n",
    "enforce_single_class = True # overwrites multiple classes and groups all instances as one\n",
    "cross_validation_split = [5,0]\n",
    "\n",
    "all_files = [f for f in listdir(dataset_location) if isfile(join(dataset_location, f))]\n",
    "\n",
    "# next, sort files into images, depth maps, segmentation maps, data, and colony info\n",
    "# we only need the location and name of the data files, as all passes follow the same naming convention\n",
    "dataset_data = []\n",
    "dataset_img = []\n",
    "dataset_ID = []\n",
    "dataset_depth = []\n",
    "dataset_norm = []\n",
    "dataset_colony = None\n",
    "\n",
    "for file in all_files:\n",
    "    loc = dataset_location + \"/\" + file\n",
    "    file_info = file.split(\"_\")\n",
    "    \n",
    "    if file_info[1] == \"BatchData\":\n",
    "        dataset_colony = loc\n",
    "        \n",
    "    elif len(file_info) == 2:\n",
    "        # images are available in various formats, but annotation data is always written as json files\n",
    "        if file_info[-1].split(\".\")[-1] == \"json\":\n",
    "            dataset_data.append(loc)\n",
    "        else:\n",
    "            dataset_img.append(loc)\n",
    "            \n",
    "    elif file_info[2].split(\".\")[0] == \"ID\":\n",
    "        dataset_ID.append(loc)\n",
    "    elif file_info[2].split(\".\")[0]  == \"depth\":\n",
    "        dataset_depth.append(loc)\n",
    "    elif file_info[2].split(\".\")[0]  == \"norm\":\n",
    "        dataset_norm.append(loc)\n",
    "        \n",
    "print(\"Found\",len(dataset_data),\"samples...\")\n",
    "\n",
    "# next sort the colony info into its IDs to determine the colony size and individual scales\n",
    "# Opening colony (BatchData) JSON file\n",
    "colony_file = open(dataset_colony)\n",
    " \n",
    "# returns JSON object as a dictionary\n",
    "colony_data = json.load(colony_file)\n",
    "colony_file.close()\n",
    "    \n",
    "colony = colony_data['BatchData']\n",
    "\n",
    "\n",
    "\"\"\" !!! requires IDs, model names, scales !!! \"\"\"\n",
    "\n",
    "\n",
    "if not enforce_single_class:\n",
    "    # get provided classes to create a dictionary of class IDs and class names\n",
    "    subject_class_names = np.unique(np.array(colony[\"weight\"]))\n",
    "    subject_classes = {}\n",
    "    for id,sbj in enumerate(subject_class_names):\n",
    "        subject_classes[str(sbj)] = id\n",
    "else:\n",
    "    subject_class_names = np.array([0])\n",
    "    subject_classes = {\"insect\" : 0}\n",
    "\n",
    "print(\"\\nA total of\",len(subject_class_names),\"unique classes have been found.\")\n",
    "print(\"The classes and respective class IDs are:\\n\",subject_classes)\n",
    "\n",
    "\n",
    "print(\"Loaded colony file with seed\", colony['seed']) #,\"and\",len(colony['ID']),\"individuals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 threads to parse data...\n",
      "Successfully created all required files!\n",
      "Starting Thread_0\n",
      "0\n",
      "{'1': {'2DBounds': {'xmin': 158.42929077148438, 'xmax': 296.8424987792969, 'ymin': 653.7243041992188, 'ymax': 751.2763671875}, 'Head': {'2DPos': {'x': 254.6927651744732, 'y': 673.6330527143705}, '3DPos': {'x': 28.373439560602975, 'y': -13.595224789030272, 'z': 37.244016290046154}}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Legos\\anaconda3\\envs\\tf-gpu\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-12-00b9584e9cfc>\", line 17, in run\n",
      "    process_detections(self.name, self.q)\n",
      "  File \"<ipython-input-12-00b9584e9cfc>\", line 61, in process_detections\n",
      "    fontColor = (int(ID_colours[int(individual[0]),0]),\n",
      "KeyError: 0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-00b9584e9cfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;31m# Wait for queue to empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mworkQueue_stacking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\queue.py\u001b[0m in \u001b[0;36mempty\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m         '''\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmutex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def getThreads():\n",
    "    \"\"\" Returns the number of available threads on a posix/win based system \"\"\"\n",
    "    if sys.platform == 'win32':\n",
    "        return int(os.environ['NUMBER_OF_PROCESSORS'])\n",
    "    else:\n",
    "        return int(os.popen('grep -c cores /proc/cpuinfo').read())\n",
    "\n",
    "class customThread(threading.Thread):\n",
    "    def __init__(self, threadID, name, q):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.threadID = threadID\n",
    "        self.name = name\n",
    "        self.q = q\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Starting \" + self.name)\n",
    "        process_detections(self.name, self.q)\n",
    "        print(\"Exiting \" + self.name)\n",
    "        \n",
    "def createThreadList(num_threads):\n",
    "    threadNames = []\n",
    "    for t in range(num_threads):\n",
    "        threadNames.append(\"Thread_\" + str(t))\n",
    "\n",
    "    return threadNames\n",
    "\n",
    "def process_detections(threadName, q):\n",
    "    while not exitFlag_stacking:\n",
    "        queueLock.acquire()\n",
    "        if not workQueue_stacking.empty():\n",
    "            \n",
    "            data_input = q.get()\n",
    "            i, data_loc, img, ID = data_input\n",
    "            print(i)\n",
    "            queueLock.release()\n",
    "            \n",
    "            display_img = cv2.imread(img)\n",
    "            display_img_out = display_img.copy()\n",
    "            \n",
    "            # compute visibility for each individual from ID pass\n",
    "            seg_img = cv2.imread(ID)\n",
    "            \n",
    "            data_file = open(data_loc)\n",
    "            # returns JSON object as a dictionary\n",
    "            data = json.load(data_file)\n",
    "            data_file.close()\n",
    "\n",
    "            if generate_dataset:\n",
    "                img_info = []\n",
    "            \n",
    "            # check if the size of the image and segmentation pass match\n",
    "            if display_img.shape != seg_img.shape:\n",
    "                print(\"Size mismatch of image and segmentation pass for sample\",data_input[1].split(\"/\")[-1],\"!\")\n",
    "            else:\n",
    "                individual_visible = False\n",
    "                \n",
    "                for individual in data[\"iterationData\"][\"subject Data\"]:\n",
    "                    ind_ID = int(individual.keys()[0])\n",
    "                    \n",
    "                    print(individual)\n",
    "\n",
    "                    fontColor = (int(ID_colours[ind_ID,0]),\n",
    "                                 int(ID_colours[ind_ID,1]),\n",
    "                                 int(ID_colours[ind_ID,2]))\n",
    "                    \n",
    "                    bbox_orig = [individual[\"2DBounds\"][\"xmin\"],\n",
    "                                 individual[\"2DBounds\"][\"ymin\"],\n",
    "                                 individual[\"2DBounds\"][\"xmax\"],\n",
    "                                 individual[\"2DBounds\"][\"ymax\"]]\n",
    "                    \n",
    "                    if enforce_tight_bboxes:\n",
    "                        bbox = fix_bounding_boxes(individual, max_val=display_img.shape)\n",
    "                    else:\n",
    "                        bbox = fix_bounding_boxes(bbox_orig, max_val=display_img.shape)\n",
    "                        \n",
    "                    # only process an individual if its bounding box width and height are not zero\n",
    "                    if bbox[2] - bbox[0] == 0 or bbox[3] - bbox[1] == 0:\n",
    "                        continue\n",
    "\n",
    "                    # FOR SOME REASON, OCCASIONALLY, THE ID OF THE SEG FILE IS LOWER THAN THE DATA FILE\n",
    "                    # with: ID = red_channel/255 * 50\n",
    "                    # red_channel = (ID/50) * 255\n",
    "                    ID_colour_val = int((ind_ID/len(colony['ID']))*255)\n",
    "                    try:\n",
    "                        ID_mask = cv2.inRange(seg_img[bbox[1]:bbox[3],bbox[0]:bbox[2]], np.array([0,0, ID_red_val - 5]), np.array([0,0, ID_red_val + 5]))\n",
    "                        indivual_occupancy = cv2.countNonZero(ID_mask)\n",
    "                    except:\n",
    "                        if len(threadList_stacking) == 1: \n",
    "                            print(\"Individual fully occluded:\",ind_ID,\"in\",dataset_seg[i])\n",
    "                        indivual_occupancy = 1\n",
    "\n",
    "                    #indivual_occupancy = np.count_nonzero((seg_img == [0, 0, int((individual[0]/len(colony['ID']))*255)]).all(axis = 2)) + np.count_nonzero((seg_img == [0, 0, int((individual[0]/len(colony['ID']))*255 - 1)]).all(axis = 2)) + np.count_nonzero((seg_img == [0, 0, int((individual[0]/len(colony['ID']))*255 + 1)]).all(axis = 2))\n",
    "                    bbox_area = abs((bbox[2] - bbox[0]) * (bbox[3] - bbox[1])) + 1\n",
    "                    bbox_occupancy = indivual_occupancy / bbox_area\n",
    "                    #print(\"Individual\", individual[0], \"with bounding box occupancy \",bbox_occupancy)\n",
    "\n",
    "                    cv2.putText(display_img, \"ID: \" + str(int(individual[0])), (bbox[0] + 10,bbox[3] - 10), font, fontScale, fontColor, lineType)\n",
    "                    if not enforce_single_class:\n",
    "                        class_ID = subject_classes[colony['weight'][int(individual[0])]]\n",
    "                    else:\n",
    "                        # here we use a single class, otherwise this can be replaced by size / scale values\n",
    "                        class_ID = 0\n",
    "\n",
    "                    if bbox_occupancy > visibility_threshold:\n",
    "\n",
    "                        individual_visible = True\n",
    "\n",
    "                        if generate_dataset:\n",
    "                            # now we need to convert the bounding box info into the desired format.\n",
    "                            img_dim = display_img.shape\n",
    "\n",
    "                            # [class_ID, centre_x, centre_y, bounding_box_width, bounding_box_height]\n",
    "                            valid_new_x = False\n",
    "                            valid_new_y = False\n",
    "\n",
    "                            if enforce_centred_bboxes:\n",
    "                                # coords of head\n",
    "                                b_t = np.array([individual[5], individual[6]])\n",
    "                                # coords of abdomen\n",
    "                                b_a_1 = np.array([individual[10], individual[11]])\n",
    "                                # compute new centre point\n",
    "                                new_centre_x = (individual[5] + individual[10]) / 2\n",
    "                                new_centre_y = (individual[6] + individual[11]) / 2\n",
    "\n",
    "                                if new_centre_x < img_dim[1] and new_centre_x > 0:\n",
    "                                    centre_x = new_centre_x / img_dim[1]\n",
    "                                    valid_new_x = True\n",
    "\n",
    "                                if new_centre_y < img_dim[0] and new_centre_y > 0:\n",
    "                                    centre_y = new_centre_y / img_dim[0]\n",
    "                                    valid_new_y = True\n",
    "\n",
    "                                cv2.circle(display_img, (int(new_centre_x),int(new_centre_y)), \n",
    "                                           radius=3, color=fontColor, thickness=-1)    \n",
    "\n",
    "                            for label in range(int((len(individual)-5)/5)):\n",
    "                                cv2.circle(display_img, (int(individual[label*5+5]),\n",
    "                                                         int(individual[label*5+6])), \n",
    "                                           radius=3, color=fontColor, thickness=-1)    \n",
    "\n",
    "\n",
    "                            bounding_box_width = abs(bbox[2] - bbox[0]) / img_dim[1]\n",
    "                            bounding_box_height = abs(bbox[3] - bbox[1]) / img_dim[0]\n",
    "\n",
    "                            if not valid_new_x or not valid_new_y:\n",
    "                                centre_x = bbox[0] / img_dim[1] + bounding_box_width / 2\n",
    "                                centre_y = bbox[1] / img_dim[0] + bounding_box_height / 2\n",
    "\n",
    "                            img_info.append([class_ID,centre_x,centre_y,bounding_box_width,bounding_box_height])\n",
    "\n",
    "                            cv2.rectangle(display_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), fontColor, 2)\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "                        if len(threadList_stacking) == 1: \n",
    "                            print(\"Ah shit, can't see\",int(individual[0]),class_ID)\n",
    "\n",
    "                    if generate_dataset and individual_visible:\n",
    "\n",
    "                        img_name = target_dir + \"/data/obj/\" + img.split('/')[-1][:-4] + \"_synth\" + \".JPG\"\n",
    "                        cv2.imwrite(img_name, display_img_out)\n",
    "\n",
    "                        with open(target_dir + \"/data/obj/\" + img.split('/')[-1][:-4] + \"_synth\" + \".txt\", \"w\") as f: \n",
    "                            output_txt = []\n",
    "                            if img_info:\n",
    "                                for line in img_info:\n",
    "                                    line_str = ' '.join([str(i) for i in line])\n",
    "                                    output_txt.append(line_str+\"\\n\")\n",
    "                                f.writelines(output_txt)\n",
    "                            else:\n",
    "                                f.write(\"\")\n",
    "\n",
    "                if len(threadList_stacking) == 1:\n",
    "                    cv2.imshow(\"labeled image\", cv2.resize(display_img, (int(display_img.shape[1] / 2), \n",
    "                                                                         int(display_img.shape[0] / 2))))\n",
    "                    cv2.waitKey(0)\n",
    "\n",
    "        else:\n",
    "            queueLock.release()\n",
    "            \n",
    "            \n",
    "####################################################################################################\n",
    "            \n",
    "# set True to show processing results for each image (disables parallel processing)\n",
    "DEBUG = True\n",
    "\n",
    "# we can additionally plot the points in the data files to check joint locations\n",
    "# WARNING: At the moment there seems to be an issue with inccorrectly given joint locations\n",
    "plot_joints = False\n",
    "\n",
    "# remember to refine an export folder when saving out your dataset\n",
    "generate_dataset = True\n",
    "\n",
    "# we can enforce the bounding box to centre on the individual instead of being influenced by its orientation\n",
    "# As the groundtruth in real recordings is annotated in the same way this should boost the average accuracy\n",
    "enforce_centred_bboxes = False\n",
    "\n",
    "# alternatively, we can draw tighter bounding boxes without enforced centres, based on 2D keypoints\n",
    "enforce_tight_bboxes = False # centred OR tight. This option will overwrite \"enforce_centred\" if True\n",
    "\n",
    "####################################################################################################\n",
    "            \n",
    "# setup as many threads as there are (virtual) CPUs\n",
    "exitFlag_stacking = 0\n",
    "if DEBUG:\n",
    "    threadList_stacking = createThreadList(1)\n",
    "else:\n",
    "    threadList_stacking = createThreadList(getThreads())\n",
    "print(\"Using\", len(threadList_stacking), \"threads to parse data...\")\n",
    "queueLock = threading.Lock()\n",
    "\n",
    "# define paths to all images and set the maximum number of items in the queue equivalent to the number of images\n",
    "workQueue_stacking = queue.Queue(len(dataset_data))\n",
    "threads = []\n",
    "threadID = 1\n",
    "\n",
    "\n",
    "np.random.seed(seed=1)\n",
    "# once colony size can be read from the BatchData file, set the size of ID_colours equal to the colony size\n",
    "ID_colours = np.random.randint(255, size=(200, 3))\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale = 0.5\n",
    "lineType = 2\n",
    "\n",
    "def fix_bounding_boxes(coords,max_val=[1024,1024],exclude_wings=True):\n",
    "    # fix bounding box coordinates so they do not reach beyond the image\n",
    "    # you can either pass only bounding box coordinates or the entire skeleton coordinates\n",
    "    # The latter will recalculate a tighter bounding box, based on all keypoints\n",
    "    # When recalculating the bounding box based on all keypoints, you can chose to ignore wings.\n",
    "    fixed_coords = []\n",
    "    \n",
    "    if len(coords) == 4:\n",
    "        coords_bbox = coords[:4]\n",
    "    \n",
    "    else:\n",
    "        coords_bbox = [0,0,max_val[0],max_val[1]]\n",
    "        # get all X and Y coordinates to find min and max values for the bounding box\n",
    "        key_x = coords[4::5]\n",
    "        key_y = coords[5::5]\n",
    "        \n",
    "        coords_bbox[0] = max([0,min(key_x)])\n",
    "        coords_bbox[1] = max([0,min(key_y)])\n",
    "        coords_bbox[2] = min([max_val[0],max(key_x)])\n",
    "        coords_bbox[3] = min([max_val[1],max(key_y)])\n",
    "    \n",
    "    for c, coord in enumerate(coords_bbox):\n",
    "        if c == 0 or c == 2:\n",
    "            max_val_temp = max_val[1]\n",
    "        else:\n",
    "            max_val_temp = max_val[0]\n",
    "            \n",
    "        if coord >= max_val_temp:\n",
    "            coord = max_val_temp\n",
    "        elif coord <= 0:\n",
    "            coord = 0\n",
    "        \n",
    "        fixed_coords.append(int(coord))\n",
    "        \n",
    "    return fixed_coords\n",
    "\n",
    "if generate_dataset:\n",
    "    from helper.Generate_YOLO_training import createCustomFiles\n",
    "    createCustomFiles(output_folder=target_dir+\"/\",obIDs=subject_class_names, k_fold=cross_validation_split)\n",
    "\n",
    "# determine the proportion of a bounding box that needs to be filled before considering the visibility as too low\n",
    "# WARNING: At the moment the ID shown in segmentation maps does not always correspond to the ID in the data file (off by 1)\n",
    "visibility_threshold = 0.015\n",
    "\n",
    "timer = time.time()\n",
    "\n",
    "# Create new threads\n",
    "for tName in threadList_stacking:\n",
    "    thread = customThread(threadID, tName, workQueue_stacking)\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    threadID += 1\n",
    "\n",
    "# Fill the queue with stacks\n",
    "queueLock.acquire()\n",
    "for i, (data, img, ID) in enumerate(zip(dataset_data , dataset_img, dataset_ID)):\n",
    "    workQueue_stacking.put([i, data, img, ID])\n",
    "queueLock.release()\n",
    "\n",
    "# Wait for queue to empty\n",
    "while not workQueue_stacking.empty():\n",
    "    pass\n",
    "\n",
    "# Notify threads it's time to exit\n",
    "exitFlag_stacking = 1\n",
    "\n",
    "# Wait for all threads to complete\n",
    "for t in threads:\n",
    "    t.join()\n",
    "print(\"Exiting Main Stacking Thread\")\n",
    "\n",
    "# close all windows if they were opened\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Total time elapsed:\",time.time()-timer,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
