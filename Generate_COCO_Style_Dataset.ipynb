{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pathlib\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 files...\n",
      "reading ColonieInfo.csv\n",
      "Loaded colony file with seed  123 and 10 individuals.\n",
      "\n",
      "A total of 4 unique classes have been found.\n",
      "The classes and respective class IDs are:\n",
      " {'00011-atta': 0, '00262-atta': 1, '00501-atta': 2, 'Sungaya-inexpectata': 3}\n"
     ]
    }
   ],
   "source": [
    "# define location of dataset and return all files\n",
    "dataset_location = \"example_data/_input_multi/\"\n",
    "target_dir = \"example_data/COCO/\"\n",
    "all_files = [f for f in listdir(dataset_location) if isfile(join(dataset_location, f))]\n",
    "\n",
    "# next, sort files into images, depth maps, segmentation maps, data, and colony info\n",
    "dataset_img = []\n",
    "dataset_depth = []\n",
    "dataset_seg = []\n",
    "dataset_data = []\n",
    "dataset_colony = dataset_location + \"/ColonieInfo.csv\"\n",
    "\n",
    "for file in all_files:\n",
    "    loc = dataset_location + \"/\" + file\n",
    "    if file[-7:-4] == \"Img\":\n",
    "        dataset_img.append(loc)\n",
    "    elif file[-7:-4] == \"Seg\":\n",
    "        dataset_seg.append(loc)\n",
    "    elif file[-9:-4] == \"Depth\":\n",
    "        dataset_depth.append(loc)\n",
    "    elif file[-8:-4] == \"Data\":\n",
    "        dataset_data.append(loc)\n",
    "        \n",
    "print(\"Found\",len(all_files),\"files...\")\n",
    "\n",
    "# next sort the colony info into its IDs to determine the colony size and individual scales\n",
    "# one entry for each successive ID is read\n",
    "from csv import reader\n",
    "\n",
    "colony = {'seed': 0,\n",
    "            'ID': [],\n",
    "         'scale': [],\n",
    "        'weight': []}\n",
    "\n",
    "with open(dataset_colony, 'r') as colony_file:\n",
    "        print(\"reading\", file)\n",
    "        # pass the file object to reader() to get the reader object\n",
    "        csv_reader = reader(colony_file)\n",
    "        # iterate over each row in the csv using reader object\n",
    "        for r, row in enumerate(csv_reader):\n",
    "            if r == 0:\n",
    "                colony['seed'] = row[0].split(\"=\")[-1]\n",
    "            else:\n",
    "                colony['ID'].append(row[0].split(\"=\")[-1])\n",
    "                colony['weight'].append(row[1].split(\"_\")[1] + \"-\" + row[1].split(\"_\")[2])\n",
    "                colony['scale'].append(float(row[2].split(\"=\")[-1]))\n",
    "\n",
    "print(\"Loaded colony file with seed\",colony['seed'],\"and\",len(colony['ID']),\"individuals.\")\n",
    "\n",
    "# get provided classes to create a dictionary of class IDs and class names\n",
    "subject_class_names = np.unique(np.array(colony[\"weight\"]))\n",
    "subject_classes = {}\n",
    "for id,sbj in enumerate(subject_class_names):\n",
    "    subject_classes[str(sbj)] = id\n",
    "\n",
    "print(\"\\nA total of\",len(subject_class_names),\"unique classes have been found.\")\n",
    "print(\"The classes and respective class IDs are:\\n\",subject_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the cleaned colony info, we can start loading the data associated with each frame.\n",
    "For simplicity we will simply this a list of list as the number of individuals.\n",
    "\n",
    "We will therefore access \"data\" as [frame] [individual] [attribute], where attributes will include [ID,bbox_x_0,bbox_y_0,...]\n",
    "\n",
    "for now training and evaluating detectors, only the bounding box (and ID) info will be relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading example_data/_input_multi//10_Data.csv\n",
      "reading example_data/_input_multi//1_Data.csv\n",
      "reading example_data/_input_multi//2_Data.csv\n",
      "reading example_data/_input_multi//3_Data.csv\n",
      "reading example_data/_input_multi//4_Data.csv\n",
      "reading example_data/_input_multi//5_Data.csv\n",
      "reading example_data/_input_multi//6_Data.csv\n",
      "reading example_data/_input_multi//7_Data.csv\n",
      "reading example_data/_input_multi//8_Data.csv\n",
      "reading example_data/_input_multi//9_Data.csv\n",
      "\n",
      "The dataset has a total of 10 generated frames.\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for file in dataset_data:\n",
    "    # store all returned coordinates for each individual\n",
    "    coords = []\n",
    "        \n",
    "    # open file in read mode\n",
    "    with open(file, 'r') as read_obj:\n",
    "        print(\"reading\", file)\n",
    "        # pass the file object to reader() to get the reader object\n",
    "        csv_reader = reader(read_obj)\n",
    "        # iterate over each row in the csv using reader object\n",
    "        for row in csv_reader:\n",
    "            # exclude camera projection row\n",
    "            if not row[0].split(\".\")[0] == \"camera_projection:\":\n",
    "                individual = [float(row[0].split(\".\")[0])]\n",
    "                # row variable is a list that represents a row in csv\n",
    "                for elem in row:\n",
    "                    try:\n",
    "                        individual.append(float(elem.split(\"=\")[-1]))\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                coords.append(individual)\n",
    "        \n",
    "    data.append(coords)\n",
    "    \n",
    "print(\"\\nThe dataset has a total of\", len(data),\"generated frames.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there may be animals for which we don't use all bones we can return a list of all labels and exclude the respective locations from the pose data. As all animals use the same convention, we can simply read in one example and remove the corresponding indices from all animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading example_data/_input_multi//9_Data.csv\n",
      "\n",
      "Corresponding to the following indices: [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287]\n",
      "\n",
      "All labels:\n",
      "['0.BoundingBox.BoundMin.X', 'BoundingBox.BoundMin.Y', 'BoundingBox.BoundMax.X', 'BoundingBox.BoundMax.Y', 'b_t.X', 'b_t.Y', 'b_t.X_world', 'b_t.Y_world', 'b_t.Z_world', 'b_a_1.X', 'b_a_1.Y', 'b_a_1.X_world', 'b_a_1.Y_world', 'b_a_1.Z_world', 'b_a_2.X', 'b_a_2.Y', 'b_a_2.X_world', 'b_a_2.Y_world', 'b_a_2.Z_world', 'b_a_3.X', 'b_a_3.Y', 'b_a_3.X_world', 'b_a_3.Y_world', 'b_a_3.Z_world', 'b_a_4.X', 'b_a_4.Y', 'b_a_4.X_world', 'b_a_4.Y_world', 'b_a_4.Z_world', 'b_a_5.X', 'b_a_5.Y', 'b_a_5.X_world', 'b_a_5.Y_world', 'b_a_5.Z_world', 'b_a_5_end.X', 'b_a_5_end.Y', 'b_a_5_end.X_world', 'b_a_5_end.Y_world', 'b_a_5_end.Z_world', 'l_1_co_r.X', 'l_1_co_r.Y', 'l_1_co_r.X_world', 'l_1_co_r.Y_world', 'l_1_co_r.Z_world', 'l_1_tr_r.X', 'l_1_tr_r.Y', 'l_1_tr_r.X_world', 'l_1_tr_r.Y_world', 'l_1_tr_r.Z_world', 'l_1_fe_r.X', 'l_1_fe_r.Y', 'l_1_fe_r.X_world', 'l_1_fe_r.Y_world', 'l_1_fe_r.Z_world', 'l_1_ti_r.X', 'l_1_ti_r.Y', 'l_1_ti_r.X_world', 'l_1_ti_r.Y_world', 'l_1_ti_r.Z_world', 'l_1_ta_r.X', 'l_1_ta_r.Y', 'l_1_ta_r.X_world', 'l_1_ta_r.Y_world', 'l_1_ta_r.Z_world', 'l_1_pt_r.X', 'l_1_pt_r.Y', 'l_1_pt_r.X_world', 'l_1_pt_r.Y_world', 'l_1_pt_r.Z_world', 'l_1_pt_r_end.X', 'l_1_pt_r_end.Y', 'l_1_pt_r_end.X_world', 'l_1_pt_r_end.Y_world', 'l_1_pt_r_end.Z_world', 'l_2_co_r.X', 'l_2_co_r.Y', 'l_2_co_r.X_world', 'l_2_co_r.Y_world', 'l_2_co_r.Z_world', 'l_2_tr_r.X', 'l_2_tr_r.Y', 'l_2_tr_r.X_world', 'l_2_tr_r.Y_world', 'l_2_tr_r.Z_world', 'l_2_fe_r.X', 'l_2_fe_r.Y', 'l_2_fe_r.X_world', 'l_2_fe_r.Y_world', 'l_2_fe_r.Z_world', 'l_2_ti_r.X', 'l_2_ti_r.Y', 'l_2_ti_r.X_world', 'l_2_ti_r.Y_world', 'l_2_ti_r.Z_world', 'l_2_ta_r.X', 'l_2_ta_r.Y', 'l_2_ta_r.X_world', 'l_2_ta_r.Y_world', 'l_2_ta_r.Z_world', 'l_2_pt_r.X', 'l_2_pt_r.Y', 'l_2_pt_r.X_world', 'l_2_pt_r.Y_world', 'l_2_pt_r.Z_world', 'l_2_pt_r_end.X', 'l_2_pt_r_end.Y', 'l_2_pt_r_end.X_world', 'l_2_pt_r_end.Y_world', 'l_2_pt_r_end.Z_world', 'l_3_co_r.X', 'l_3_co_r.Y', 'l_3_co_r.X_world', 'l_3_co_r.Y_world', 'l_3_co_r.Z_world', 'l_3_tr_r.X', 'l_3_tr_r.Y', 'l_3_tr_r.X_world', 'l_3_tr_r.Y_world', 'l_3_tr_r.Z_world', 'l_3_fe_r.X', 'l_3_fe_r.Y', 'l_3_fe_r.X_world', 'l_3_fe_r.Y_world', 'l_3_fe_r.Z_world', 'l_3_ti_r.X', 'l_3_ti_r.Y', 'l_3_ti_r.X_world', 'l_3_ti_r.Y_world', 'l_3_ti_r.Z_world', 'l_3_ta_r.X', 'l_3_ta_r.Y', 'l_3_ta_r.X_world', 'l_3_ta_r.Y_world', 'l_3_ta_r.Z_world', 'l_3_pt_r.X', 'l_3_pt_r.Y', 'l_3_pt_r.X_world', 'l_3_pt_r.Y_world', 'l_3_pt_r.Z_world', 'l_3_pt_r_end.X', 'l_3_pt_r_end.Y', 'l_3_pt_r_end.X_world', 'l_3_pt_r_end.Y_world', 'l_3_pt_r_end.Z_world', 'w_1_r.X', 'w_1_r.Y', 'w_1_r.X_world', 'w_1_r.Y_world', 'w_1_r.Z_world', 'w_1_r_end.X', 'w_1_r_end.Y', 'w_1_r_end.X_world', 'w_1_r_end.Y_world', 'w_1_r_end.Z_world', 'w_2_r.X', 'w_2_r.Y', 'w_2_r.X_world', 'w_2_r.Y_world', 'w_2_r.Z_world', 'w_2_r_end.X', 'w_2_r_end.Y', 'w_2_r_end.X_world', 'w_2_r_end.Y_world', 'w_2_r_end.Z_world', 'l_1_co_l.X', 'l_1_co_l.Y', 'l_1_co_l.X_world', 'l_1_co_l.Y_world', 'l_1_co_l.Z_world', 'l_1_tr_l.X', 'l_1_tr_l.Y', 'l_1_tr_l.X_world', 'l_1_tr_l.Y_world', 'l_1_tr_l.Z_world', 'l_1_fe_l.X', 'l_1_fe_l.Y', 'l_1_fe_l.X_world', 'l_1_fe_l.Y_world', 'l_1_fe_l.Z_world', 'l_1_ti_l.X', 'l_1_ti_l.Y', 'l_1_ti_l.X_world', 'l_1_ti_l.Y_world', 'l_1_ti_l.Z_world', 'l_1_ta_l.X', 'l_1_ta_l.Y', 'l_1_ta_l.X_world', 'l_1_ta_l.Y_world', 'l_1_ta_l.Z_world', 'l_1_pt_l.X', 'l_1_pt_l.Y', 'l_1_pt_l.X_world', 'l_1_pt_l.Y_world', 'l_1_pt_l.Z_world', 'l_1_pt_l_end.X', 'l_1_pt_l_end.Y', 'l_1_pt_l_end.X_world', 'l_1_pt_l_end.Y_world', 'l_1_pt_l_end.Z_world', 'l_2_co_l.X', 'l_2_co_l.Y', 'l_2_co_l.X_world', 'l_2_co_l.Y_world', 'l_2_co_l.Z_world', 'l_2_tr_l.X', 'l_2_tr_l.Y', 'l_2_tr_l.X_world', 'l_2_tr_l.Y_world', 'l_2_tr_l.Z_world', 'l_2_fe_l.X', 'l_2_fe_l.Y', 'l_2_fe_l.X_world', 'l_2_fe_l.Y_world', 'l_2_fe_l.Z_world', 'l_2_ti_l.X', 'l_2_ti_l.Y', 'l_2_ti_l.X_world', 'l_2_ti_l.Y_world', 'l_2_ti_l.Z_world', 'l_2_ta_l.X', 'l_2_ta_l.Y', 'l_2_ta_l.X_world', 'l_2_ta_l.Y_world', 'l_2_ta_l.Z_world', 'l_2_pt_l.X', 'l_2_pt_l.Y', 'l_2_pt_l.X_world', 'l_2_pt_l.Y_world', 'l_2_pt_l.Z_world', 'l_2_pt_l_end.X', 'l_2_pt_l_end.Y', 'l_2_pt_l_end.X_world', 'l_2_pt_l_end.Y_world', 'l_2_pt_l_end.Z_world', 'l_3_co_l.X', 'l_3_co_l.Y', 'l_3_co_l.X_world', 'l_3_co_l.Y_world', 'l_3_co_l.Z_world', 'l_3_tr_l.X', 'l_3_tr_l.Y', 'l_3_tr_l.X_world', 'l_3_tr_l.Y_world', 'l_3_tr_l.Z_world', 'l_3_fe_l.X', 'l_3_fe_l.Y', 'l_3_fe_l.X_world', 'l_3_fe_l.Y_world', 'l_3_fe_l.Z_world', 'l_3_ti_l.X', 'l_3_ti_l.Y', 'l_3_ti_l.X_world', 'l_3_ti_l.Y_world', 'l_3_ti_l.Z_world', 'l_3_ta_l.X', 'l_3_ta_l.Y', 'l_3_ta_l.X_world', 'l_3_ta_l.Y_world', 'l_3_ta_l.Z_world', 'l_3_pt_l.X', 'l_3_pt_l.Y', 'l_3_pt_l.X_world', 'l_3_pt_l.Y_world', 'l_3_pt_l.Z_world', 'l_3_pt_l_end.X', 'l_3_pt_l_end.Y', 'l_3_pt_l_end.X_world', 'l_3_pt_l_end.Y_world', 'l_3_pt_l_end.Z_world', 'w_1_l.X', 'w_1_l.Y', 'w_1_l.X_world', 'w_1_l.Y_world', 'w_1_l.Z_world', 'w_1_l_end.X', 'w_1_l_end.Y', 'w_1_l_end.X_world', 'w_1_l_end.Y_world', 'w_1_l_end.Z_world', 'w_2_l.X', 'w_2_l.Y', 'w_2_l.X_world', 'w_2_l.Y_world', 'w_2_l.Z_world', 'w_2_l_end.X', 'w_2_l_end.Y', 'w_2_l_end.X_world', 'w_2_l_end.Y_world', 'w_2_l_end.Z_world', 'b_h.X', 'b_h.Y', 'b_h.X_world', 'b_h.Y_world', 'b_h.Z_world', 'ma_r.X', 'ma_r.Y', 'ma_r.X_world', 'ma_r.Y_world', 'ma_r.Z_world', 'ma_r_end.X', 'ma_r_end.Y', 'ma_r_end.X_world', 'ma_r_end.Y_world', 'ma_r_end.Z_world', 'an_1_r.X', 'an_1_r.Y', 'an_1_r.X_world', 'an_1_r.Y_world', 'an_1_r.Z_world', 'an_2_r.X', 'an_2_r.Y', 'an_2_r.X_world', 'an_2_r.Y_world', 'an_2_r.Z_world', 'an_3_r.X', 'an_3_r.Y', 'an_3_r.X_world', 'an_3_r.Y_world', 'an_3_r.Z_world', 'an_3_r_end.X', 'an_3_r_end.Y', 'an_3_r_end.X_world', 'an_3_r_end.Y_world', 'an_3_r_end.Z_world', 'ma_l.X', 'ma_l.Y', 'ma_l.X_world', 'ma_l.Y_world', 'ma_l.Z_world', 'ma_l_end.X', 'ma_l_end.Y', 'ma_l_end.X_world', 'ma_l_end.Y_world', 'ma_l_end.Z_world', 'an_1_l.X', 'an_1_l.Y', 'an_1_l.X_world', 'an_1_l.Y_world', 'an_1_l.Z_world', 'an_2_l.X', 'an_2_l.Y', 'an_2_l.X_world', 'an_2_l.Y_world', 'an_2_l.Z_world', 'an_3_l.X', 'an_3_l.Y', 'an_3_l.X_world', 'an_3_l.Y_world', 'an_3_l.Z_world', 'an_3_l_end.X', 'an_3_l_end.Y', 'an_3_l_end.X_world', 'an_3_l_end.Y_world', 'an_3_l_end.Z_world']\n"
     ]
    }
   ],
   "source": [
    "# first open and read the first line from the first imported data file\n",
    "labels = []\n",
    "with open(dataset_data[0], 'r') as read_obj:\n",
    "    print(\"reading\", file)\n",
    "    # pass the file object to reader() to get the reader object\n",
    "    csv_reader = reader(read_obj)\n",
    "    row_0 = next(csv_reader)  # gets the first line\n",
    "    # iterate over each row in the csv using reader object\n",
    "    for elem in row_0:\n",
    "        try:\n",
    "            labels.append((elem.split(\"=\")[0].split(\"Bone.\")[-1]))\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "# now let's define which labels NOT to use (in our case, all labels relating to wings)\n",
    "# ... so that just means \"omit all lables that start with 'w'\"\n",
    "\n",
    "matched_labels = []# [i for i, item in enumerate(labels) if item in omit_labels]\n",
    "for l, label in enumerate(labels):\n",
    "    if label[0] == \"w\":\n",
    "        matched_labels.append(l-1)\n",
    "        \n",
    "print(\"\\nCorresponding to the following indices:\",matched_labels)\n",
    "\n",
    "# show all used labels:\n",
    "print(\"\\nAll labels:\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded data and colony info we can start plotting bounding boxes on top of their respective images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform between sRGB and linear colour space (optional)\n",
    "\n",
    "def to_linear(srgb):\n",
    "    linear = np.float32(srgb) / 255.0\n",
    "    less = linear <= 0.04045\n",
    "    linear[less] = linear[less] / 12.92\n",
    "    linear[~less] = np.power((linear[~less] + 0.055) / 1.055, 2.4)\n",
    "    return linear * 255.0\n",
    "\n",
    "    \n",
    "def from_linear(linear):\n",
    "    srgb = linear.copy()\n",
    "    less = linear <= 0.0031308\n",
    "    srgb[less] = linear[less] * 12.92\n",
    "    srgb[~less] = 1.055 * np.power(linear[~less], 1.0 / 2.4) - 0.055\n",
    "    return srgb * 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# let's create a big dictionary to store all our dataset info and\n",
    "# then dump it into a sexy COCO-conform json file\n",
    "# based on the documentation : https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch\n",
    "coco_data = {}\n",
    "\n",
    "from datetime import datetime\n",
    "date = datetime.today().strftime('%d.%m.%Y')\n",
    "\n",
    "# edit any \"info\", \"license\", or \"category\" data here:\n",
    "coco_data[\"info\"] = {\n",
    "        \"description\": \"COCO_Style_FARTS_example_dataset\",\n",
    "        \"url\": \"https://evo-biomech.ic.ac.uk/\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"year\": datetime.today().year,\n",
    "        \"contributor\": \"Fabian Plum, Rene Bulla, David Labonte\",\n",
    "        \"date_created\": date}\n",
    "\n",
    "coco_data[\"licenses\"] = [\n",
    "        {\n",
    "            \"url\": \"http://creativecommons.org/licenses/by/4.0/\",\n",
    "            \"id\": 1,\n",
    "            \"name\": \"Attribution License\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "coco_data[\"categories\"] = []\n",
    "\n",
    "for s,sbj in enumerate(subject_class_names):\n",
    "    coco_data[\"categories\"].append(\n",
    "            {\n",
    "                \"supercategory\": \"insect\",\n",
    "                \"id\": s + 1,\n",
    "                \"name\": sbj,\n",
    "                \"keypoints\":['b_t', 'b_a_1', 'b_a_2', 'b_a_3',\n",
    "                             'b_a_4', 'b_a_5', 'b_a_5_end', 'l_1_co_r',\n",
    "                             'l_1_tr_r', 'l_1_fe_r',  'l_1_ti_r', 'l_1_ta_r', \n",
    "                             'l_1_pt_r', 'l_1_pt_r_end', 'l_2_co_r', 'l_2_tr_r', \n",
    "                             'l_2_fe_r', 'l_2_ti_r', 'l_2_ta_r', 'l_2_pt_r', \n",
    "                             'l_2_pt_r_end', 'l_3_co_r', 'l_3_tr_r', 'l_3_fe_r', \n",
    "                             'l_3_ti_r', 'l_3_ta_r', 'l_3_pt_r', 'l_3_pt_r_end',\n",
    "                             'w_1_r', 'w_1_r_end',  'w_2_r', 'w_2_r_end',\n",
    "                             'l_1_co_l', 'l_1_tr_l', 'l_1_fe_l', 'l_1_ti_l',\n",
    "                             'l_1_ta_l', 'l_1_pt_l', 'l_1_pt_l_end', 'l_2_co_l', \n",
    "                             'l_2_tr_l', 'l_2_fe_l', 'l_2_ti_l', 'l_2_ta_l',\n",
    "                             'l_2_pt_l', 'l_2_pt_l_end', 'l_3_co_l', 'l_3_tr_l',\n",
    "                             'l_3_fe_l', 'l_3_ti_l', 'l_3_ta_l', 'l_3_pt_l',\n",
    "                             'l_3_pt_l_end', 'w_1_l', 'w_1_l_end', 'w_2_l',\n",
    "                             'w_2_l_end', 'b_h', 'ma_r', 'ma_r_end',\n",
    "                             'an_1_r', 'an_2_r', 'an_3_r', 'an_3_r_end',\n",
    "                             'ma_l', 'ma_l_end', 'an_1_l', 'an_2_l', \n",
    "                             'an_3_l', 'an_3_l_end'],\n",
    "                \"skeleton\":[\n",
    "                    [2,1],[3,2],[4,3],\n",
    "                    [5,4],[6,5],[7,6],\n",
    "                    [9,8],[10,9],[11,10],[12,11],\n",
    "                    [13,12],[14,13],[16,15],\n",
    "                    [17,16],[18,17],[19,18],[20,19],\n",
    "                    [21,20],[23,22],[24,23],\n",
    "                    [25,24],[26,25],[27,26],[28,27],\n",
    "                    [30,29],[32,31],\n",
    "                    [34,33],[35,34],[36,35],\n",
    "                    [37,36],[38,37],[39,38],\n",
    "                    [41,40],[42,41],[43,42],[44,43],\n",
    "                    [45,44],[46,45],[48,47],\n",
    "                    [49,48],[50,49],[51,50],[52,51],\n",
    "                    [53,52],[55,54],\n",
    "                    [57,56],[58,1],[59,58],[60,59],\n",
    "                    [61,58],[62,61],[63,62],[64,63],\n",
    "                    [65,58],[66,65],[67,58],[68,67],\n",
    "                    [69,68],[70,69]\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "# when adding images in the next step the following info needs to be given:\n",
    "coco_data[\"images\"] = []\n",
    "\n",
    "# FORMATTING NOTES [\"images\"]\n",
    "\n",
    "\"\"\"\n",
    "\"images\": [\n",
    "    {\n",
    "        \"id\": ###### (-> generated ID, use i, needs to the same for annoations),\n",
    "        \"license\": 1,\n",
    "        \"width\": display_img.shape[0],\n",
    "        \"height\": display_img.shape[0],\n",
    "        \"file_name\": img.split('/')[-1][:-4] + \"_synth\" + \".JPG\"\n",
    "    },\n",
    "    ...\n",
    "\"\"\"\n",
    "\n",
    "# FORMATTING NOTES [\"annotations\"]\n",
    "\n",
    "\"\"\"\n",
    "\"annotations\": [\n",
    "    {\n",
    "        \"segmentation\": [[x0,y0,x1,y1...xn,yn][x_0,y_0,...x_n,y_n]] (-> coordinates of mask outline, if seperated, multiple arrays can be passed),\n",
    "        \"area\": #### (-> = to the sum of pixels inside the mask),\n",
    "        \"iscrowd\": 0 (as we treat all individuals seperately),\n",
    "        \"image_id\": # (-> = i when iterating over all images),\n",
    "        \"bbox\": [bbox[0], bbox[1], bbox[2]-bbox[0], bbox[3]-bbox[1]] (-> unlike darknet the original (sub-)pixel values are used here),\n",
    "        \"category_id\": 1 (-> for now there is only one category, replace with class ID for multi class),\n",
    "        \"id\": ##### (-> separate counter to i and im)\n",
    "    },\n",
    "\"\"\"\n",
    "\n",
    "# each individual in the dataset is treated as a sparate annotation with a corresponding image ID\n",
    "coco_data[\"annotations\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 48 threads for export...\n",
      "Starting Thread_0\n",
      "Starting Thread_1\n",
      "Starting Thread_2\n",
      "Starting Thread_3\n",
      "Starting Thread_4\n",
      "Starting Thread_5\n",
      "Starting Thread_6\n",
      "Starting Thread_7\n",
      "Starting Thread_8\n",
      "Starting Thread_9\n",
      "Starting Thread_10Starting Thread_11\n",
      "\n",
      "Starting Thread_12\n",
      "Starting Thread_13\n",
      "Starting Thread_14\n",
      "Starting Thread_15\n",
      "Starting Thread_16\n",
      "Starting Thread_17\n",
      "Starting Thread_18\n",
      "Starting Thread_19\n",
      "Starting Thread_20\n",
      "Starting Thread_21\n",
      "Starting Thread_22Starting Thread_23\n",
      "Starting Thread_24\n",
      "\n",
      "Starting Thread_25\n",
      "Starting Thread_26\n",
      "Starting Thread_27\n",
      "Starting Thread_28\n",
      "Starting Thread_29\n",
      "Starting Thread_30\n",
      "Starting Thread_31\n",
      "Starting Thread_32\n",
      "Starting Thread_33\n",
      "Starting Thread_34Starting Thread_35\n",
      "\n",
      "Starting Thread_36\n",
      "Starting Thread_37\n",
      "Starting Thread_38\n",
      "Starting Thread_39\n",
      "Starting Thread_40\n",
      "Starting Thread_41\n",
      "Starting Thread_42\n",
      "Starting Thread_43Starting Thread_44\n",
      "\n",
      "Starting Thread_45\n",
      "Starting Thread_46\n",
      "Starting Thread_47\n",
      "Exiting Thread_30Exiting Thread_2\n",
      "\n",
      "Exiting Thread_22\n",
      "Exiting Thread_47Exiting Thread_28\n",
      "\n",
      "Exiting Thread_6\n",
      "Exiting Thread_33\n",
      "Exiting Thread_35\n",
      "Exiting Thread_38Exiting Thread_44Exiting Thread_29\n",
      "Exiting Thread_42\n",
      "\n",
      "\n",
      "Exiting Thread_17\n",
      "Exiting Thread_24\n",
      "Exiting Thread_41\n",
      "Exiting Thread_1\n",
      "Exiting Thread_11\n",
      "Exiting Thread_13\n",
      "Exiting Thread_3Exiting Thread_43Exiting Thread_4\n",
      "\n",
      "Exiting Thread_14\n",
      "Exiting Thread_36\n",
      "Exiting Thread_25\n",
      "Exiting Thread_15\n",
      "Exiting Thread_45Exiting Thread_21\n",
      "\n",
      "Exiting Thread_34\n",
      "Exiting Thread_23\n",
      "Exiting Thread_31\n",
      "\n",
      "Exiting Thread_7Exiting Thread_26Exiting Thread_12\n",
      "\n",
      "Exiting Thread_39\n",
      "Exiting Thread_19\n",
      "\n",
      "Exiting Thread_46\n",
      "Exiting Thread_8Exiting Thread_18\n",
      "\n",
      "Exiting Thread_10\n",
      "Saved example_data/COCO//data/3_Img_synth.jpg\n",
      "Exiting Thread_37\n",
      "Saved example_data/COCO//data/9_Img_synth.jpg\n",
      "Exiting Thread_20\n",
      "Saved example_data/COCO//data/8_Img_synth.jpg\n",
      "Exiting Thread_16\n",
      "Saved example_data/COCO//data/4_Img_synth.jpg\n",
      "Exiting Thread_27\n",
      "Saved example_data/COCO//data/1_Img_synth.jpg\n",
      "Exiting Thread_0\n",
      "Saved example_data/COCO//data/10_Img_synth.jpg\n",
      "Exiting Thread_5\n",
      "Saved example_data/COCO//data/7_Img_synth.jpg\n",
      "Exiting Thread_40\n",
      "Saved example_data/COCO//data/6_Img_synth.jpgSaved example_data/COCO//data/2_Img_synth.jpg\n",
      "Exiting Thread_32\n",
      "\n",
      "Exiting Thread_9\n",
      "Exiting Main export Thread\n",
      "Total time elapsed: 0.6949272155761719 seconds\n"
     ]
    }
   ],
   "source": [
    "# create unique colours for each ID\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# alright. Let's take it from the top and fucking multi-thread this.\n",
    "import threading\n",
    "import queue\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def getThreads():\n",
    "    \"\"\" Returns the number of available threads on a posix/win based system \"\"\"\n",
    "    if sys.platform == 'win32':\n",
    "        return int(os.environ['NUMBER_OF_PROCESSORS'])\n",
    "    else:\n",
    "        return int(os.popen('grep -c cores /proc/cpuinfo').read())\n",
    "\n",
    "class exportThread(threading.Thread):\n",
    "    def __init__(self, threadID, name, q):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.threadID = threadID\n",
    "        self.name = name\n",
    "        self.q = q\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Starting \" + self.name)\n",
    "        process_detections(self.name, self.q)\n",
    "        print(\"Exiting \" + self.name)\n",
    "        \n",
    "def createThreadList(num_threads):\n",
    "    threadNames = []\n",
    "    for t in range(num_threads):\n",
    "        threadNames.append(\"Thread_\" + str(t))\n",
    "\n",
    "    return threadNames\n",
    "\n",
    "def process_detections(threadName, q):\n",
    "    while not exitFlag_export:\n",
    "        queueLock.acquire()\n",
    "        if not workQueue_export.empty():\n",
    "            \n",
    "            data_input = q.get()\n",
    "            i = data_input[0]\n",
    "            img = data_input[1]\n",
    "            queueLock.release()\n",
    "            \n",
    "            display_img = cv2.imread(img)\n",
    "            display_img_orig = display_img.copy()\n",
    "            \n",
    "            # only add images that contain visibile individuals\n",
    "            is_empty = True\n",
    "            \n",
    "            img_name = target_dir + \"/data/\" + img.split('/')[-1][:-4] + \"_synth\" + \".jpg\"\n",
    "\n",
    "            img_info = []\n",
    "                \n",
    "            # compute visibility for each individual\n",
    "            seg_img = cv2.imread(dataset_seg[i])\n",
    "            seg_img_display = seg_img.copy()\n",
    "\n",
    "            for im, individual in enumerate(data[i]):\n",
    "\n",
    "                fontColor = (int(ID_colours[int(individual[0]),0]),\n",
    "                             int(ID_colours[int(individual[0]),1]),\n",
    "                             int(ID_colours[int(individual[0]),2]))\n",
    "                bbox = fix_bounding_boxes(individual[1:5],max_val=display_img.shape)\n",
    "\n",
    "                # FOR SOME REASON OCCASIONALLY THE ID OF THE SEG FILE IS LOWER THAN THE DATA FILE\n",
    "                # with: ID = red_channel/255 * im\n",
    "                # red_channel = (ID/im) * 255\n",
    "                ID_red_val = int((individual[0]/len(colony['ID']))*255)\n",
    "                \n",
    "                contours_lowpoly = []\n",
    "                \n",
    "                try:\n",
    "                    ID_mask = cv2.inRange(seg_img[bbox[1]:bbox[3],bbox[0]:bbox[2]], np.array([0,0, ID_red_val - 2]), np.array([0,0, ID_red_val + 2]))\n",
    "                    indivual_occupancy = cv2.countNonZero(ID_mask)\n",
    "                    \n",
    "                    # the kernel size for both dilation and median blur are to be determined by the bbounding boxes relative size\n",
    "                    rel_size = ((bbox[2] - bbox[0]) / display_img.shape[0] + (bbox[3] - bbox[1]) / display_img.shape[0]) / 2\n",
    "                    # values range from 0 (tiny) to 1 (huge)\n",
    "                    # required smoothing 5 to 95\n",
    "                    rel_size_root = int(round((15 * rel_size)/2.)*2 + 1) # round to next odd integer\n",
    "                    #print(\"img:\", i, \"individual:\", im, \"rel_size\", rel_size, rel_size_root)\n",
    "\n",
    "                    # to simplify the generated masks and counter compression artifacts the original mask is dilated\n",
    "                    # https://docs.opencv.org/3.4/db/df6/tutorial_erosion_dilatation.html\n",
    "                    kernel = np.ones((rel_size_root, rel_size_root), 'uint8')\n",
    "                    ID_mask_dilated = cv2.dilate(ID_mask, kernel, iterations=1)\n",
    "                    # use median blur to further smooth the edges of the binary mask\n",
    "                    ID_mask_dilated = cv2.medianBlur(ID_mask_dilated,rel_size_root)\n",
    "\n",
    "                    # pad segmentation subwindow to prevent contours from being cut off\n",
    "                    \"\"\"\n",
    "                    pad_width = 20\n",
    "                    ID_mask_dilated_padded = np.zeros([ID_mask_dilated.shape[0] + pad_width * 2 , ID_mask_dilated.shape[1] + pad_width * 2], 'uint8')\n",
    "                    ID_mask_dilated_padded[pad_width:-pad_width,pad_width:-pad_width] = ID_mask_dilated\n",
    "                    \"\"\"\n",
    "\n",
    "                    # find contours using cv2.CHAIN_APPROX_SIMPLE to minimise the number of control points\n",
    "                    # use cv2.RETR_EXTERNAL instead of cv2.RETR_TREE to only return the outer most contours\n",
    "                    # depending on the version of openCV the function findContours additionally returns the image\n",
    "                    try:\n",
    "                        contours, hierarchy = cv2.findContours(ID_mask_dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)\n",
    "                    except:\n",
    "                        useless_img, contours, hierarchy = cv2.findContours(ID_mask_dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)\n",
    "                    # now sort contours by area and only keep the 4 largest parts \n",
    "                    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "                    if len(contours) > 3:\n",
    "                        contours = contours[:4]\n",
    "\n",
    "                    # finally we simplify the generated contours to decrease memory usage\n",
    "                    # and fascilitate correct processing, using polygon approximation\n",
    "                    for contour in contours:\n",
    "                        # decrease epsilon for finer contours\n",
    "                        contours_lowpoly.append(cv2.approxPolyDP(contour, epsilon=1, closed=True))\n",
    "\n",
    "\n",
    "                    if len(threadList_export) == 1:\n",
    "                        print(\"\\nindividual\",im,ID_mask_dilated.dtype)\n",
    "                        print(hierarchy)\n",
    "                        # draw the contours on the empty image\n",
    "                        seg_img_display = seg_img.copy()\n",
    "                        cv2.imshow(\"mask: \", ID_mask_dilated)\n",
    "                        cv2.drawContours(seg_img_display[bbox[1]:bbox[3],bbox[0]:bbox[2]], contours, -1, (255,0,0), 3)\n",
    "                        cv2.imshow(\"segmentation: \", seg_img_display[bbox[1]:bbox[3],bbox[0]:bbox[2]])\n",
    "                        cv2.waitKey(1)\n",
    "                        \n",
    "                except:\n",
    "                    indivual_occupancy = 1\n",
    "                \n",
    "                #indivual_occupancy = np.count_nonzero((seg_img == [0, 0, int((individual[0]/len(colony['ID']))*255)]).all(axis = 2)) + np.count_nonzero((seg_img == [0, 0, int((individual[0]/len(colony['ID']))*255 - 1)]).all(axis = 2)) + np.count_nonzero((seg_img == [0, 0, int((individual[0]/len(colony['ID']))*255 + 1)]).all(axis = 2))\n",
    "                bbox_area = abs((bbox[2] - bbox[0]) * (bbox[3] - bbox[1])) + 1\n",
    "                bbox_occupancy = indivual_occupancy / bbox_area\n",
    "                #print(\"Individual\", individual[0], \"with bounding box occupancy \",bbox_occupancy)\n",
    "                \n",
    "                class_ID = subject_classes[colony['weight'][int(individual[0])]] # here we use a single class, otherwise this can be replaced by size / scale values\n",
    "                \n",
    "                #cv2.putText(display_img, \"ID: \" + str(int(individual[0])), (bbox[0] + 10,bbox[3] - 10), font, fontScale, fontColor, lineType)\n",
    "                if bbox_occupancy > visibility_threshold:\n",
    "                    #cv2.rectangle(display_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), fontColor, 2)\n",
    "                    \n",
    "                    # collect all joint info and convert into COCO readable format\n",
    "                    # \"keypoints\" are arrays of length 3K, K is the total number of key points defined for a class \n",
    "                    # [x, y, v] with the key point visibility v:\n",
    "\n",
    "                    # v=0   Indicates that this key point is not marked (in this case x=y=v=0）\n",
    "                    # v=1   Indicates that this key point is marked but not visible(Obscured)\n",
    "                    # v=2   Indicates that this key point is marked and visible at the same time\n",
    "                    \n",
    "                    # let's binarise the image and dilate it to make sure all points that are visible are found\n",
    "                    seg_bin = cv2.inRange(seg_img, np.array([0,0, ID_red_val - 2]), np.array([0,245, ID_red_val + 2]))\n",
    "                    kernel = np.ones((9,9),np.uint8)\n",
    "                    seg_bin_dilated = cv2.dilate(seg_bin,kernel,iterations = 1)\n",
    "        \n",
    "                    \n",
    "                    keypoints = []\n",
    "                    img_shape = display_img.shape\n",
    "                    for point in range(int(len(individual[5:])/5)):  \n",
    "                        # check if point is located within the image\n",
    "                        if point*5 + 4 in matched_labels or individual[point*5 + 5] > img_shape[0] or individual[point*5 + 5] < 0 or individual[point*5 + 6] > img_shape[1] or individual[point*5 + 6] < 0:\n",
    "                            keypoints.extend([0,0,0]) # x=y=v=0 -> ignore keypoint\n",
    "                        else:\n",
    "                            # if it is, check its visibility\n",
    "                            if seg_bin_dilated[int(individual[6 + point*5]),int(individual[5 + point*5])] == 255:                   \n",
    "                                visibility_pt = 2 # point is visible\n",
    "                            else:\n",
    "                                visibility_pt = 1 # point is marked but obstructed\n",
    "                       \n",
    "                            keypoints.extend([int(individual[point*5 + 5]),int(individual[point*5 + 6]),visibility_pt])\n",
    "                        #cv2.circle(display_img, (int(individual[point*2 + 5]),int(individual[point*2 + 6])), radius=3, color=fontColor, thickness=-1)\n",
    "                        # let's see of this is really the centre\n",
    "                        \n",
    "                            \n",
    "                            \n",
    "                    if generate_dataset:\n",
    "                        # now we need to convert all the info into the desired format.\n",
    "                        segmentation_mask= []\n",
    "                        \n",
    "                        new_bbox = [display_img.shape[0],display_img.shape[1],0,0]\n",
    "                        mask_area = 0\n",
    "                        \n",
    "                        if len(contours_lowpoly) != 0:\n",
    "                            for contour in contours_lowpoly:\n",
    "                                mask_area += cv2.contourArea(contour)\n",
    "                                sub_mask = []\n",
    "                                for coords in contour:\n",
    "                                    sub_mask_x = int(bbox[0] + coords[0,0])\n",
    "                                    sub_mask_y = int(bbox[1] + coords[0,1])\n",
    "                                    sub_mask.append(sub_mask_x)\n",
    "                                    sub_mask.append(sub_mask_y)\n",
    "\n",
    "                                    if sub_mask_x < new_bbox[0]:\n",
    "                                        new_bbox[0] = sub_mask_x\n",
    "                                    if sub_mask_x > new_bbox[2]:\n",
    "                                        new_bbox[2] = sub_mask_x\n",
    "\n",
    "                                    if sub_mask_y < new_bbox[1]:\n",
    "                                        new_bbox[1] = sub_mask_y\n",
    "                                    if sub_mask_y > new_bbox[3]:\n",
    "                                        new_bbox[3] = sub_mask_y\n",
    "\n",
    "                                if len(sub_mask) >= 8:\n",
    "                                    # only include polygons with at least 4 vertices\n",
    "                                    segmentation_mask.append(sub_mask)\n",
    "                                    is_empty = False\n",
    "\n",
    "                        # now that we have a clean segmentation mask, we can refine the bounding box as well\n",
    "                        \n",
    "                        if not is_empty:\n",
    "                            coco_data[\"annotations\"].append({\n",
    "                                    \"segmentation\": segmentation_mask, # (-> coordinates of mask outline, if seperated, multiple arrays can be passed),\n",
    "                                    \"area\": mask_area, # (-> = to the sum of pixels inside the mask),\n",
    "                                    \"iscrowd\": 0, #(as we treat all individuals seperately),\n",
    "                                    \"image_id\":  i, # (-> = i when iterating over all images),\n",
    "                                    \"bbox\": [new_bbox[0], new_bbox[1], new_bbox[2]-new_bbox[0], new_bbox[3]-new_bbox[1]], # (-> unlike darknet the original (sub-)pixel values are used here),\n",
    "                                    \"category_id\": class_ID + 1, # (-> for now there is only one category),\n",
    "                                    \"id\": int(str(i) + \"000\" + str(im)), # (-> joining i and im)\n",
    "                                    \"keypoints\": keypoints\n",
    "                                    })\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "                    # create mask to highlight low visibility animals\n",
    "                    #blk = np.zeros(display_img.shape, np.uint8)\n",
    "                    #cv2.rectangle(blk, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "                    # display original bounding box\n",
    "                    #cv2.rectangle(display_img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), fontColor, 2)\n",
    "                    # add text to discarded ID\n",
    "                    #cv2.putText(display_img, \"OCCLUDED\", (bbox[0] + 10,bbox[3] - 35), font, fontScale, (0,0,255), lineType)\n",
    "\n",
    "                    # blend the mask with original image\n",
    "                    #display_img = cv2.addWeighted(display_img, 1.0, blk, 0.25, 1)\n",
    "\n",
    "                    #print(\"Individual\", int(individual[0]), \"has been discarded due to excessive occlusion.\")\n",
    "                    #print(\"expected:\",int((individual[0]/len(colony['ID']))*255))\n",
    "                    \n",
    "                            \n",
    "            \n",
    "            # uncomment to show resulting bounding boxes and masks\n",
    "            if len(threadList_export) == 1:\n",
    "                cv2.imshow(\"segmentation: \" ,cv2.resize(seg_img_display, (int(seg_img.shape[1] / 2), \n",
    "                                                                  int(seg_img.shape[0] / 2))))\n",
    "                cv2.imshow(\"labeled image\", cv2.resize(display_img, (int(display_img.shape[1] / 2), \n",
    "                                                                     int(display_img.shape[0] / 2))))\n",
    "                cv2.waitKey(1)\n",
    "            \n",
    "            \n",
    "            if not is_empty:\n",
    "                coco_data[\"images\"].append({\n",
    "                        \"id\": i,\n",
    "                        \"license\": 1,\n",
    "                        \"width\": display_img.shape[0],\n",
    "                        \"height\": display_img.shape[1],\n",
    "                        \"file_name\": img.split('/')[-1][:-4] + \"_synth\" + \".JPG\"\n",
    "                    }\n",
    "                )\n",
    "                cv2.imwrite(img_name, display_img)\n",
    "                print(\"Saved\", img_name)\n",
    "            \n",
    "        else:\n",
    "            queueLock.release()\n",
    "            \n",
    "# setup as many threads as there are (virtual) CPUs\n",
    "exitFlag_export = 0\n",
    "# only use a fourth of the number of CPUs for export as hugin and enfuse utilise multi core processing in part\n",
    "threadList_export = createThreadList(getThreads() * 4)\n",
    "print(\"Using\", len(threadList_export), \"threads for export...\")\n",
    "queueLock = threading.Lock()\n",
    "\n",
    "# define paths to all images and set the maximum number of items in the queue equivalent to the number of images\n",
    "workQueue_export = queue.Queue(len(dataset_img))\n",
    "threads = []\n",
    "threadID = 1\n",
    "\n",
    "np.random.seed(seed=1)\n",
    "ID_colours = np.random.randint(255, size=(len(colony['ID']), 3))\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "fontScale = 0.5\n",
    "lineType = 2\n",
    "\n",
    "# we can additionally plot the points in the data files to check joint locations\n",
    "# WARNING: At the moment there seems to be an issue with inccorrectly given joint locations\n",
    "plot_joints = True\n",
    "\n",
    "# remember to refine an export folder when saving out your dataset\n",
    "generate_dataset = True\n",
    "\n",
    "def fix_bounding_boxes(coords,max_val = [1024,1024]):\n",
    "    # fix bounding box coordinates so they do not reach beyond the image\n",
    "    fixed_coords = []\n",
    "    for c, coord in enumerate(coords):\n",
    "        if c == 0 or c == 2:\n",
    "            max_val_temp = max_val[0]\n",
    "        else:\n",
    "            max_val_temp = max_val[1]\n",
    "            \n",
    "        if coord >= max_val_temp:\n",
    "            coord = max_val_temp\n",
    "        elif coord <= 0:\n",
    "            coord = 0\n",
    "        \n",
    "        fixed_coords.append(int(coord))\n",
    "        \n",
    "    return fixed_coords\n",
    "\n",
    "# determine the proportion of a bounding box that needs to be filled before considering the visibility as too low\n",
    "# WARNING: At the moment the ID shown in segmentation maps does not always correspond to the ID in the data file (off by 1)\n",
    "visibility_threshold = 0.015\n",
    "\n",
    "timer = time.time()\n",
    "\n",
    "# create output folder for used images\n",
    "if not os.path.exists(target_dir + \"/data\"):\n",
    "    os.mkdir(target_dir + \"/data\")\n",
    "\n",
    "# Create new threads\n",
    "for tName in threadList_export:\n",
    "    thread = exportThread(threadID, tName, workQueue_export)\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    threadID += 1\n",
    "\n",
    "# Fill the queue with stacks\n",
    "queueLock.acquire()\n",
    "for i,img in enumerate(dataset_img):\n",
    "    workQueue_export.put([i, img])\n",
    "queueLock.release()\n",
    "\n",
    "# Wait for queue to empty\n",
    "while not workQueue_export.empty():\n",
    "    pass\n",
    "\n",
    "# Notify threads it's time to exit\n",
    "exitFlag_export = 1\n",
    "\n",
    "# Wait for all threads to complete\n",
    "for t in threads:\n",
    "    t.join()\n",
    "print(\"Exiting Main export Thread\")\n",
    "\n",
    "# close all windows if they were opened\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Total time elapsed:\",time.time()-timer,\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, dump it all into one sexy **COCO style json** file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(target_dir + '/labels.json', 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(coco_data, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
